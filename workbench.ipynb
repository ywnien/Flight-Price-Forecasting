{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from functools import cache\n",
    "from typing import Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from numpy.typing import ArrayLike\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from tqdm import tqdm\n",
    "\n",
    "sns.set_theme()\n",
    "\n",
    "LAGS = 14\n",
    "N_STEPS = 7\n",
    "VALIDATION_FOLDS = 5\n",
    "\n",
    "\n",
    "def convert_timestamps(df: pd.DataFrame):\n",
    "    df[\"searchDate\"] = pd.to_datetime(df[\"searchDate\"])\n",
    "    df[\"segmentsDepartureTimeEpochSeconds\"] = pd.to_datetime(\n",
    "        df[\"segmentsDepartureTimeEpochSeconds\"],\n",
    "        unit=\"s\"\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_and_sort(df: pd.DataFrame):\n",
    "    # Tickets with duration smaller than `LAGS + N_STEPS` were filtered\n",
    "    group = df.groupby(\"legId\")[\"searchDate\"]\n",
    "    duration = group.max() - group.min() + pd.Timedelta(days=1)\n",
    "    duration = duration[duration >= pd.Timedelta(days=LAGS+N_STEPS)]\n",
    "    leg_valid = duration.index\n",
    "    df = df[df[\"legId\"].isin(leg_valid)]\n",
    "    # sort searchDate by their minimum date and then maximum date\n",
    "    sorted_idx = np.lexsort((group.max()[leg_valid], group.min()[leg_valid]))\n",
    "    sorted_legs = leg_valid[sorted_idx]\n",
    "\n",
    "    return (\n",
    "        df.set_index(\"legId\")\n",
    "        .loc[sorted_legs]\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "\n",
    "def leg_slice_generator(df: pd.DataFrame):\n",
    "    pos = 0\n",
    "    for span in df[\"legId\"].value_counts(sort=False):\n",
    "        yield slice(pos, pos+span)\n",
    "        pos += span\n",
    "\n",
    "\n",
    "def ffill_bfill(arr: np.ndarray):\n",
    "    \"\"\"\n",
    "    Peforming foward filling and backward filling. The index of elements would be \n",
    "    created, and the indexes of nan values were set to 0.Then, doing accumulate \n",
    "    maximum to find the mamimum available indexes at current position. Finally, \n",
    "    get the filled array by the resulting indexes.\n",
    "    \"\"\"\n",
    "    mask = np.isnan(arr)\n",
    "    # return array of row indexes and the index of nan is set to 0\n",
    "    idx = np.where(~mask, np.arange(arr.shape[0])[:, None], 0)\n",
    "    np.maximum.accumulate(idx, axis=0, out=idx)\n",
    "    # calcuating accumulate maximum in reverse order is back filling\n",
    "    np.maximum.accumulate(idx[::-1], axis=0, out=idx)\n",
    "    idx = idx[::-1]\n",
    "    return arr[idx, np.arange(arr.shape[1])]\n",
    "\n",
    "\n",
    "def impute_null_data(df: pd.DataFrame):\n",
    "    arr = df[[\"segmentsEquipmentDescription\", \"totalTravelDistance\"]].to_numpy()\n",
    "    enc = OrdinalEncoder()\n",
    "    arr = enc.fit_transform(arr)\n",
    "\n",
    "    for leg_slice in leg_slice_generator(df):\n",
    "        subarr = arr[leg_slice]\n",
    "        if np.isnan(subarr).any() and np.isnan(subarr).all() != True:\n",
    "            subarr[:] = ffill_bfill(subarr)\n",
    "\n",
    "    arr = enc.inverse_transform(arr)\n",
    "    df[\"segmentsEquipmentDescription\"] = arr[:, 0]\n",
    "    df[\"totalTravelDistance\"] = arr[:, 1].astype(np.float32)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_distance(df: pd.DataFrame):\n",
    "    distance_airport_map = {\"ONT\": 1897, \"LAX\": 1943}\n",
    "    df[\"totalTravelDistance\"].fillna(\n",
    "        df[\"segmentsArrivalAirportCode\"].map(distance_airport_map),\n",
    "        inplace=True\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def fill_equipment(df: pd.DataFrame):\n",
    "    null = df[\"segmentsEquipmentDescription\"].isna()\n",
    "    spirit = df[\"segmentsAirlineName\"] == \"Spirit Airlines\"\n",
    "    delta = df[\"segmentsAirlineName\"] == \"Delta\"\n",
    "    df.loc[null & spirit, \"segmentsEquipmentDescription\"] = \"AIRBUS INDUSTRIE A320 SHARKLETS\"\n",
    "    df.loc[null & delta, \"segmentsEquipmentDescription\"] = \"Airbus A321\"   \n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def impute_lost_days(df: pd.DataFrame):\n",
    "    # date_features = [\n",
    "    #     \"searchDate\",\n",
    "    #     \"segmentsDepartureTimeEpochSeconds\",\n",
    "    # ]\n",
    "    # str_features = [\n",
    "    #     \"legId\",\n",
    "    #     \"fareBasisCode\",\n",
    "    #     \"segmentsArrivalAirportCode\",\n",
    "    #     \"segmentsAirlineName\",\n",
    "    #     \"segmentsEquipmentDescription\",\n",
    "    # ]\n",
    "    # num_features = [\n",
    "    #     \"totalFare\",\n",
    "    #     \"seatsRemaining\",\n",
    "    #     \"isBasicEconomy\",\n",
    "    #     \"totalTravelDistance\",\n",
    "    #     \"segmentsDurationInSeconds\",\n",
    "    # ]\n",
    "\n",
    "    df_arr = df.to_numpy()\n",
    "    search_date_index = df.columns.get_loc(\"searchDate\")\n",
    "    search_date = df_arr[:, search_date_index].astype(\"datetime64[D]\")\n",
    "\n",
    "    # losing data or changing legId caused the searchDate difference not equal 1\n",
    "    date_diff = np.diff(search_date) // np.timedelta64(1, \"D\")\n",
    "    # excluding the part caused by changing legId\n",
    "    leg_counts = df[\"legId\"].value_counts(sort=False)\n",
    "    leg_change_indexes = (leg_counts.to_numpy().cumsum() - 1)[:-1]\n",
    "    date_diff[leg_change_indexes] = 1\n",
    "    \n",
    "    # calculate the corresponding indexes of existing data in the new array\n",
    "    new_indexes = np.zeros(search_date.shape[0], dtype=int)\n",
    "    date_diff.cumsum(out=new_indexes[1:])\n",
    "\n",
    "    arr = np.zeros([new_indexes[-1] + 1, df.shape[1]], dtype=object)\n",
    "    arr[new_indexes] = df_arr # copy the existing data to the new array\n",
    "\n",
    "    # impute part\n",
    "    mark_arr = np.zeros(arr.shape[0], dtype=np.float32)\n",
    "    DAY = np.timedelta64(1, \"D\")\n",
    "    for i in np.nonzero(date_diff != 1)[0]:\n",
    "        lost_days = date_diff[i] - 1\n",
    "        start =  new_indexes[i] + 1\n",
    "        stop = start + lost_days\n",
    "        arr[start:stop] = df_arr[i] # forward fill\n",
    "        mark_arr[start:stop] = 1 # mark the imputed data\n",
    "        ref_day = np.datetime64(df_arr[i, search_date_index], \"D\")\n",
    "        arr[start:stop, search_date_index] = np.arange(\n",
    "            ref_day + DAY, ref_day + DAY * date_diff[i], DAY\n",
    "        )\n",
    "\n",
    "    array_dict = {\n",
    "        key: arr[:, i].astype(type_)\n",
    "        for i, (key, type_) in enumerate(df.dtypes.items())\n",
    "    }\n",
    "    array_dict.update({\"imputed\": mark_arr})\n",
    "    return array_dict\n",
    "\n",
    "\n",
    "@cache\n",
    "def month_start(year, month):\n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "\n",
    "def cyclic_encode(\n",
    "    timestamps: ArrayLike, period: Literal[\"day\", \"week\", \"month\", \"year\"]\n",
    "):\n",
    "    \"\"\"\n",
    "    Encoding features which can be converted by `pd.to_datetime`.\n",
    "    The returning features consist of sine and cosine waves with period determined\n",
    "    by the parameter `period`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    timestamps : ArrayLike object of timestamps\n",
    "        Could be converted by `pd.to_datetime`\n",
    "    period : \"day\", \"week\", \"month\" or \"year\"\n",
    "        The period in sine and cosine wave\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple(x_sin, x_cos)\n",
    "    \"\"\"\n",
    "    if not isinstance(timestamps, pd.Series):\n",
    "        timestamps = pd.Series(timestamps)\n",
    "\n",
    "    if period == \"day\":\n",
    "        offset = (timestamps - timestamps.dt.normalize()).dt.total_seconds()\n",
    "        _period = 86400\n",
    "    elif period == \"week\":\n",
    "        # seconds of days passed this week + hour, minute and seconds\n",
    "        offset = timestamps.dt.day_of_week * 86400\n",
    "        offset += (timestamps - timestamps.dt.normalize()).dt.total_seconds()\n",
    "        _period = 86400 * 7\n",
    "    elif period == \"month\":\n",
    "        # offset to the the beginning of the month\n",
    "        offset = (\n",
    "            timestamps.apply(lambda x: x - month_start(x.year, x.month))\n",
    "            .dt.total_seconds()\n",
    "        )\n",
    "        _period = timestamps.dt.days_in_month * 86400 # 86400 seconds in a day\n",
    "    elif period == \"year\":\n",
    "        offset = timestamps.dt.day_of_year * 86400\n",
    "        offset += (timestamps - timestamps.dt.normalize()).dt.total_seconds()\n",
    "        _period = 86400 * 365\n",
    "    else:\n",
    "        raise ValueError(\"The parameter period only support day, week, month and year\")\n",
    "\n",
    "    basis = 2 * np.pi * offset / _period\n",
    "    basis = basis.to_numpy(np.float32)\n",
    "    return np.sin(basis), np.cos(basis)\n",
    "\n",
    "\n",
    "\n",
    "class EncodeData:\n",
    "    target_names = (\"totalFare\", \"seatsRemaining\")\n",
    "    encoding_spec = {\n",
    "        \"Ordinal\": [\"segmentsAirlineName\", \"fareBasisCode\"],\n",
    "        \"OneHot\": [\"segmentsArrivalAirportCode\", \"segmentsEquipmentDescription\"],\n",
    "    }\n",
    "    encoders = {}\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        if df is None:\n",
    "            return\n",
    "        self.data = self.clean_impute_df(df)\n",
    "        self.metadata = {\n",
    "            \"legId\": self.data.pop(\"legId\"),\n",
    "            \"searchDate\": self.data[\"searchDate\"],\n",
    "            \"imputed\" : self.data.pop(\"imputed\"),\n",
    "        }\n",
    "        self.columns, self._x, self._y = self.encode()\n",
    "\n",
    "    @classmethod\n",
    "    def read_csv(cls, filepath: str, **kwargs):\n",
    "        return cls(pd.read_csv(filepath, **kwargs))\n",
    "\n",
    "    @staticmethod\n",
    "    def clean_impute_df(df: pd.DataFrame) -> dict[str, np.ndarray]:\n",
    "        selected_features = [\n",
    "            \"searchDate\", \"segmentsDepartureTimeEpochSeconds\", \"legId\",\n",
    "            \"fareBasisCode\", \"segmentsArrivalAirportCode\", \"segmentsAirlineName\",\n",
    "            \"segmentsEquipmentDescription\", \"totalFare\", \"seatsRemaining\",\n",
    "            \"isBasicEconomy\", \"totalTravelDistance\", \"segmentsDurationInSeconds\",\n",
    "        ]\n",
    "        return (\n",
    "            df.loc[:, selected_features]\n",
    "            .pipe(convert_timestamps)\n",
    "            .pipe(filter_and_sort)\n",
    "            .pipe(impute_null_data)\n",
    "            .pipe(fill_distance)\n",
    "            .pipe(fill_equipment)\n",
    "            .pipe(impute_lost_days)\n",
    "        )\n",
    "    \n",
    "    def fit_encoders(self):\n",
    "        search_date = self.data[\"searchDate\"]\n",
    "        last_day = search_date.max() - pd.Timedelta(days=N_STEPS * VALIDATION_FOLDS)\n",
    "        train_mask = search_date <= last_day\n",
    "\n",
    "        for feature in self.encoding_spec[\"Ordinal\"]:\n",
    "            self.encoders[feature] = OrdinalEncoder(\n",
    "                dtype=np.float32,\n",
    "                handle_unknown=\"use_encoded_value\",\n",
    "                unknown_value=-1,\n",
    "            )\n",
    "            self.encoders[feature].fit(self.data[feature][train_mask].reshape(-1, 1))\n",
    "        for feature in self.encoding_spec[\"OneHot\"]:\n",
    "            self.encoders[feature] = OneHotEncoder(\n",
    "                dtype=np.float32,\n",
    "                handle_unknown=\"infrequent_if_exist\",\n",
    "                min_frequency=0.01,\n",
    "                sparse_output=False,\n",
    "                feature_name_combiner=lambda _, category: f\"{feature}_{category}\",\n",
    "            )\n",
    "            self.encoders[feature].fit(self.data[feature][train_mask].reshape(-1, 1))\n",
    "\n",
    "    def encode(self):\n",
    "        if not self.encoders:\n",
    "            self.fit_encoders()\n",
    "\n",
    "        # Ordinal encode\n",
    "        embed_data = {}\n",
    "        for feature in self.encoding_spec[\"Ordinal\"]:\n",
    "            arr = self.data.pop(feature)\n",
    "            enc = self.encoders[feature]\n",
    "            embed_data[feature] = enc.transform(arr.reshape(-1, 1))\n",
    "        # One Hot encode\n",
    "        for feature in self.encoding_spec[\"OneHot\"]:\n",
    "            arr = self.data.pop(feature)\n",
    "            enc = self.encoders[feature]\n",
    "            arr = enc.transform(arr.reshape(-1, 1))\n",
    "            self.data.update(\n",
    "                {name: arr[:, i] for i, name in enumerate(enc.get_feature_names_out())}\n",
    "            )\n",
    "        # Cyclic encode\n",
    "        period_args = {\n",
    "            \"searchDate\": [\"week\", \"month\", \"year\"],\n",
    "            \"segmentsDepartureTimeEpochSeconds\": [\"day\", \"week\", \"month\", \"year\"],\n",
    "        }\n",
    "        for feature, period_list in period_args.items():\n",
    "            arr = self.data.pop(feature)\n",
    "            for period in period_list:\n",
    "                waves = cyclic_encode(arr, period=period)\n",
    "                descr = f\"{feature}_{period}\"\n",
    "                self.data.update({f\"{descr}_sin\": waves[0], f\"{descr}_cos\": waves[1]})\n",
    "\n",
    "        # return (columns, x, y)\n",
    "        return (\n",
    "            tuple(tuple(data.keys()) for data in (self.data, embed_data)),\n",
    "            tuple(\n",
    "                np.column_stack(tuple(data.values())).astype(np.float32)\n",
    "                for data in (self.data, embed_data)\n",
    "            ),\n",
    "            np.column_stack(tuple(self.data[name] for name in self.target_names)),\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class WindowData:\n",
    "    x: tuple[list | np.ndarray] = field(default_factory=list)\n",
    "    y: list | np.ndarray = field(default_factory=list)\n",
    "    imputed_x: list | np.ndarray = field(default_factory=list)\n",
    "    imputed_y: list | np.ndarray = field(default_factory=list)\n",
    "    date: list | np.ndarray = field(default_factory=list)\n",
    "\n",
    "    @classmethod\n",
    "    def sliding_window(cls, encoded: EncodeData):\n",
    "        window = cls(x=tuple([] for _ in encoded._x))\n",
    "        causal = cls()\n",
    "\n",
    "        search_date = encoded.metadata[\"searchDate\"].reshape(-1, 1)\n",
    "        imputed = encoded.metadata[\"imputed\"].reshape(-1, 1)\n",
    "        for leg_slice in window._leg_slice_generator(encoded.metadata[\"legId\"]):\n",
    "            # append windowed samples\n",
    "            for frag_list, arr in zip(window.x, encoded._x):\n",
    "                frag_list.append(cls._window_x(arr[leg_slice]))\n",
    "            window.y.append(cls._window_y(encoded._y[leg_slice]))\n",
    "            window.imputed_x.append(cls._window_x(imputed[leg_slice]))\n",
    "            window.imputed_y.append(cls._window_y(imputed[leg_slice]))\n",
    "            window.date.append(cls._window_y(search_date[leg_slice]))\n",
    "            # dataset for causal model only changes those related to output (y)\n",
    "            causal.y.append(cls._window_y(encoded._y[leg_slice], causal=True))\n",
    "            causal.imputed_y.append(cls._window_y(imputed[leg_slice], causal=True))\n",
    "\n",
    "        window._vstack_samples()\n",
    "        causal._vstack_samples()\n",
    "        # convert imputed counts to imputed rate (percentage)\n",
    "        window.imputed_x = cls._compute_imputed_rate(window.imputed_x)\n",
    "        window.imputed_y = cls._compute_imputed_rate(window.imputed_y)\n",
    "        causal.imputed_y = cls._compute_imputed_rate(causal.imputed_y)\n",
    "        # use last date of samples to split train, validation and test set\n",
    "        window.date = window.date[:, -1].flatten()\n",
    "        ind = np.argsort(window.date)\n",
    "        window.apply_index(ind)\n",
    "        causal.apply_index(ind)\n",
    "        # reference the attributes from `window` because they are all the same\n",
    "        causal.x = window.x\n",
    "        causal.imputed_x = window.imputed_x\n",
    "        causal.date = window.date\n",
    "\n",
    "        return window, causal\n",
    "    \n",
    "    def _vstack_samples(self):\n",
    "        for field_name in self.__dataclass_fields__:\n",
    "            attr = getattr(self, field_name)\n",
    "            if isinstance(attr, list) and attr:\n",
    "                attr = np.vstack(attr)\n",
    "            elif isinstance(attr, tuple):\n",
    "                attr = tuple(np.vstack(arr) for arr in attr)\n",
    "            else:\n",
    "                continue\n",
    "            setattr(self, field_name, attr)\n",
    "\n",
    "    @staticmethod\n",
    "    def _leg_slice_generator(leg_id_array: np.ndarray):\n",
    "        leg_counts = pd.Series(leg_id_array).value_counts(sort=False)\n",
    "        pos = 0\n",
    "        for span in leg_counts:\n",
    "            yield slice(pos, pos+span)\n",
    "            pos += span\n",
    "\n",
    "    @staticmethod\n",
    "    def _window_x(array: np.ndarray):\n",
    "        return np.lib.stride_tricks.sliding_window_view(\n",
    "            array[:-N_STEPS], window_shape=LAGS, axis=0\n",
    "        ).swapaxes(1, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _window_y(array: np.ndarray, causal: bool = False):\n",
    "        if causal:\n",
    "            return np.lib.stride_tricks.sliding_window_view(\n",
    "                array[1:], window_shape=(N_STEPS, LAGS), axis=(0, 0)\n",
    "            ).swapaxes(1, 3)\n",
    "        else:\n",
    "            return np.lib.stride_tricks.sliding_window_view(\n",
    "                array[LAGS:], window_shape=N_STEPS, axis=0\n",
    "            ).swapaxes(1, 2)\n",
    "\n",
    "    @staticmethod\n",
    "    def _compute_imputed_rate(imputed: np.ndarray):\n",
    "        axis_to_sum = tuple(range(1, imputed.ndim))\n",
    "        return imputed.sum(axis=axis_to_sum) / np.prod(imputed.shape[1:])\n",
    "    \n",
    "    def apply_index(self, ind: np.ndarray):\n",
    "        for field_name in self.__dataclass_fields__:\n",
    "            attr = getattr(self, field_name)\n",
    "            if isinstance(attr, np.ndarray):\n",
    "                attr = attr[ind]\n",
    "            elif isinstance(attr, tuple):\n",
    "                attr = tuple(arr[ind] for arr in attr)\n",
    "            else:\n",
    "                continue\n",
    "            setattr(self, field_name, attr)\n",
    "\n",
    "\n",
    "class SplitData:\n",
    "    def __init__(self, window: WindowData) -> None:\n",
    "        self.window = window\n",
    "        self._normalization_params = []\n",
    "\n",
    "    @property\n",
    "    def sample_weight(self):\n",
    "        return (1 - self.window.imputed_x) * (1 - self.window.imputed_y)\n",
    "\n",
    "    def train_valid_split(self, n_folds: int = VALIDATION_FOLDS):\n",
    "        self._normalization_params.clear()\n",
    "        x, y = self.window.x, self.window.y\n",
    "        train_boundary = self.window.date.max() - np.timedelta64(N_STEPS * n_folds, \"D\")\n",
    "        for _ in range(n_folds):\n",
    "            valid_boundary = train_boundary + np.timedelta64(N_STEPS, \"D\")\n",
    "            train_mask = self.window.date <= train_boundary\n",
    "            valid_mask = (self.window.date <= valid_boundary) ^ train_mask\n",
    "            kwargs = {\n",
    "                \"x\": [arr[train_mask] for arr in x],\n",
    "                \"y\": [arr[train_mask] for arr in np.rollaxis(y, axis=-1)],\n",
    "                \"validation_data\": (\n",
    "                    [arr[valid_mask] for arr in x],\n",
    "                    [arr[valid_mask] for arr in np.rollaxis(y, axis=-1)],\n",
    "                ),\n",
    "                \"sample_weight\": self.sample_weight[train_mask],\n",
    "            }\n",
    "            yield self._normalize_split(kwargs)\n",
    "            # yield kwargs\n",
    "            train_boundary = valid_boundary\n",
    "\n",
    "    def _normalize_split(self, split_dict):\n",
    "        params = {} # parameters are mean and std\n",
    "        dest = []\n",
    "        x_train, x_valid = split_dict[\"x\"], split_dict[\"validation_data\"][0]\n",
    "        for i, arr in enumerate(x_train):\n",
    "            norm_train, norm_valid, mean, std = self.normalize(arr, x_valid[i])\n",
    "            x_train[i], x_valid[i] = norm_train, norm_valid\n",
    "            dest.append((mean ,std))\n",
    "        params[\"x\"] = tuple(dest)\n",
    "        # only normalize the regression target: totalFare\n",
    "        y_train, y_valid = split_dict[\"y\"], split_dict[\"validation_data\"][1]\n",
    "        norm_train, norm_valid, mean, std = self.normalize(y_train[0], y_valid[0])\n",
    "        y_train[0], y_valid[0] = norm_train, norm_valid\n",
    "        params[\"y\"] = (mean, std)\n",
    "        self._normalization_params.append(params) # save params in object\n",
    "        return split_dict\n",
    "\n",
    "    @staticmethod\n",
    "    def normalize(train: np.ndarray, valid: np.ndarray):\n",
    "        # if the array only contains 1 feature, the shape would be (samples, timesteps)\n",
    "        ndim = max(train.ndim, 3) # ensure calling mean/std like 3d array\n",
    "        axes = tuple(range(ndim - 1))\n",
    "        mean = train.mean(axis=axes)\n",
    "        std = train.std(axis=axes)\n",
    "        return (train - mean) / std, (valid - mean) / std, mean, std\n",
    "\n",
    "    def denormalize_fare(self, array: np.ndarray, nth_fold: int = -1):\n",
    "        mean, std = self._normalization_params[nth_fold][\"y\"]\n",
    "        return array * std + mean\n",
    "\n",
    "\n",
    "def collapse_2d(x: np.ndarray):\n",
    "    last_x = x[:, -1]\n",
    "    # [0, 1] are indexes of totalFare and seatsRemaining\n",
    "    targets_x = x[:, :-1, [0, 1]].reshape(x.shape[0], -1)\n",
    "    return np.hstack((targets_x, last_x))\n",
    "\n",
    "\n",
    "def sklearn_model_kwargs(fold: dict, regression_only: bool = False):\n",
    "    x = collapse_2d(np.concatenate(fold[\"x\"], axis=-1))\n",
    "    x_valid = collapse_2d(np.concatenate(fold[\"validation_data\"][0], axis=-1))\n",
    "    y = fold[\"y\"]\n",
    "    if regression_only:\n",
    "        y = np.concatenate(y, axis=-1)\n",
    "        return {\n",
    "            \"fit\": ((x, y), {\"sample_weight\": fold[\"sample_weight\"]}),\n",
    "            \"predict\": {\"X\": x_valid},\n",
    "        }\n",
    "    return (\n",
    "        {\n",
    "            \"fit\": ((x, y[0]), {\"sample_weight\": fold[\"sample_weight\"]}),\n",
    "            \"predict\": {\"X\": x_valid},\n",
    "        },\n",
    "        {\n",
    "            \"fit\": ((x, y[1]), {\"sample_weight\": fold[\"sample_weight\"]}),\n",
    "            \"predict\": {\"X\": x_valid},\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "def _predict_reg_only(fold: dict, reg) -> np.ndarray:\n",
    "    sk_kwargs = sklearn_model_kwargs(fold, regression_only=True)\n",
    "    return (\n",
    "        reg.fit(*sk_kwargs[\"fit\"][0], **sk_kwargs[\"fit\"][1])\n",
    "        .predict(**sk_kwargs[\"predict\"])\n",
    "    )\n",
    "\n",
    "\n",
    "def _predict_reg_clf(fold: dict, reg, clf) -> tuple[np.ndarray]:\n",
    "    sk_kwargs = sklearn_model_kwargs(fold)\n",
    "    # cloning a new, not referenced model to make sure it is freed after prediciton\n",
    "    return tuple(\n",
    "        clone(model).fit(*kwargs[\"fit\"][0], **kwargs[\"fit\"][1])\n",
    "        .predict(**kwargs[\"predict\"])\n",
    "        for kwargs, model in zip(sk_kwargs, (reg, clf))\n",
    "    )\n",
    "\n",
    "\n",
    "def sk_model_predict(window: WindowData, name: str, reg=None, clf=None):\n",
    "    splits = SplitData(window)\n",
    "    fare_list = []\n",
    "    seat_list = []\n",
    "    for i, fold in tqdm(\n",
    "        enumerate(splits.train_valid_split()), total=VALIDATION_FOLDS, desc=name\n",
    "    ):\n",
    "        if clf is None:\n",
    "            y_pred = _predict_reg_only(fold, reg) # np.ndarray\n",
    "            fare_list.append(splits.denormalize_fare(y_pred[:, :N_STEPS], i))\n",
    "            seat_list.append(y_pred[:, N_STEPS:].round())\n",
    "        else:\n",
    "            y_pred = _predict_reg_clf(fold, reg, clf) # tuple[np.ndarray]\n",
    "            fare_list.append(splits.denormalize_fare(y_pred[0], i))\n",
    "            seat_list.append(y_pred[1])\n",
    "    np.savez_compressed(\n",
    "        f\"models/{name}.npz\", fare=np.vstack(fare_list), seat=np.vstack(seat_list)\n",
    "    )\n",
    "    print(f\"y_pred saved at 'models/{name}.npz'\")\n",
    "\n",
    "\n",
    "def load_y_pred(path: str) -> dict[str, np.ndarray]:\n",
    "    with np.load(path) as f:\n",
    "        y_preds = dict(f)\n",
    "    return y_preds\n",
    "\n",
    "\n",
    "def _melt_relplot(fare_mae: pd.DataFrame, seat_mae: pd.DataFrame):\n",
    "    fare_mae[\"day\"] = np.arange(N_STEPS) + 1\n",
    "    fare_mae[\"target\"] = \"totalFare\"\n",
    "    seat_mae[\"day\"] = np.arange(N_STEPS) + 1\n",
    "    seat_mae[\"target\"] = \"seatsRemaining\"\n",
    "    mae = pd.concat([fare_mae, seat_mae])\n",
    "    melt = pd.melt(mae, [\"day\", \"target\"], var_name=\"model\", value_name=\"MAE\")\n",
    "\n",
    "    by = [\"target\", \"model\"]\n",
    "    style = style_order = None\n",
    "    if melt[\"model\"].str.contains(\"_reg_only\").any():\n",
    "        style = \"reg_only\"\n",
    "        by.append(style)\n",
    "        style_order = [\"Yes\", \"No\"]\n",
    "        melt[\"reg_only\"] = np.where(\n",
    "            melt[\"model\"].str.contains(\"_reg_only\"), \"No\", \"Yes\"\n",
    "        )\n",
    "        melt[\"model\"] = melt[\"model\"].str.replace(\"_reg_only\", \"\")\n",
    "\n",
    "    sns.relplot(\n",
    "        melt, x=\"day\", y=\"MAE\", hue=\"model\", style=style, kind=\"line\",\n",
    "        style_order=style_order, col=\"target\", facet_kws={\"sharey\": False}\n",
    "    )\n",
    "    display(melt.drop(columns=\"day\").groupby(by).agg([\"mean\", \"max\", \"min\"]))\n",
    "\n",
    "\n",
    "def plot_mae(name_and_path: dict):\n",
    "    y_true = load_y_pred(\"models/y_true.npz\")\n",
    "    fare_mae = {}\n",
    "    seat_mae = {}\n",
    "    for name, path in name_and_path.items():\n",
    "        y_pred = load_y_pred(path)\n",
    "        y_pred[\"seat\"] = y_pred[\"seat\"].round() # TODO: make sure rounded while saving\n",
    "        for mae, target in zip((fare_mae, seat_mae), (\"fare\", \"seat\")):\n",
    "            mae[name] = mean_absolute_error(\n",
    "                y_true[target], y_pred[target], multioutput=\"raw_values\"\n",
    "            )\n",
    "    _melt_relplot(pd.DataFrame(fare_mae), pd.DataFrame(seat_mae))\n",
    "\n",
    "\n",
    "def error_dist(pred_path: str, days_avg: bool = True):\n",
    "    y_true = load_y_pred(\"models/y_true.npz\")\n",
    "    y_pred = load_y_pred(pred_path)\n",
    "    target_spec = {\"fare\": \"totalFare\", \"seat\": \"seatsRemaining\"}\n",
    "    hue = None\n",
    "    palette = None\n",
    "    if days_avg:\n",
    "        errors = {\n",
    "            name: (y_pred[key] - y_true[key]).mean(axis=-1)\n",
    "            for key, name in target_spec.items()\n",
    "        }\n",
    "        errors = pd.DataFrame(errors).melt(value_name=\"error\", var_name=\"target\")\n",
    "    else:\n",
    "        hue = \"days\"\n",
    "        palette = \"crest\"\n",
    "        errors = pd.concat(\n",
    "            pd.DataFrame(y_pred[key] - y_true[key], columns=np.arange(7) + 1)\n",
    "            .melt(value_name=\"error\", var_name=hue).assign(target=name)\n",
    "            for key, name in {\"fare\": \"totalFare\", \"seat\": \"seatsRemaining\"}.items()\n",
    "        )\n",
    "    sns.displot(\n",
    "        errors, x=\"error\", col=\"target\", kind=\"kde\", hue=hue, palette=palette,\n",
    "        facet_kws={\"sharex\": False, \"sharey\": False}\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick Start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run snapshot\n",
    "import joblib\n",
    "\n",
    "LAGS = 14\n",
    "N_STEPS = 7\n",
    "# encoded = joblib.load(\"EncodeData_encoded.gz\")\n",
    "# test_run_data = joblib.load(\"test_run_data.gz\")\n",
    "\n",
    "window = joblib.load(\"WindowData_window.gz\")\n",
    "# splits = SplitData(window)\n",
    "causal = joblib.load(\"WindowData_causal.gz\")\n",
    "# splits = SplitData(causal)\n",
    "# fold_1 = next(splits.train_valid_split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3bbdfd7d8d107824bf9970146417b842bdb83554d9d690c7452f93a78926ffc8'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import hashlib\n",
    "\n",
    "hashlib.sha256(causal.__repr__().encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from time import sleep\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def slow(n):\n",
    "    sleep(1)\n",
    "    return n\n",
    "\n",
    "Parallel(5)(delayed(slow)(i) for i in range(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### machine learning performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### extra tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "\n",
    "reg = ExtraTreesRegressor(bootstrap=False, n_jobs=14)\n",
    "clf = ExtraTreesClassifier(bootstrap=False, n_jobs=14)\n",
    "y_pred = _predict_reg_clf(fold_1, reg, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fare 0.23375716055806559\n",
      "seat 0.4416006282572999\n"
     ]
    }
   ],
   "source": [
    "for name, _true, _pred in zip((\"fare\", \"seat\"), fold_1[\"validation_data\"][1], y_pred):\n",
    "    print(name, mean_absolute_error(_true, _pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.backend import clear_session\n",
    "from keras.layers import Input, Embedding, Concatenate, Reshape\n",
    "\n",
    "clear_session()\n",
    "\n",
    "# embed_dim = 10\n",
    "# embed_dim = 4\n",
    "embed_dim = 16\n",
    "\n",
    "encoded = Input(shape=(LAGS, 34), name=\"encoded\")\n",
    "fare_basis = Input(shape=(LAGS, 1), name=\"fare_basis\")\n",
    "\n",
    "embedded = Embedding(input_dim=250, output_dim=embed_dim)(fare_basis)\n",
    "embedded = Reshape((LAGS, embed_dim))(embedded)\n",
    "concatenated_tensor = Concatenate(axis=-1)([encoded, embedded])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 14, 7) dtype=float32 (created by layer 'conv1d_1')>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import layers\n",
    "\n",
    "conv = layers.Conv1D(7, 1)\n",
    "conv(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import TimeDistributed, Dense, EinsumDense\n",
    "\n",
    "lay = TimeDistributed(Dense(N_STEPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 14, 7, 11) dtype=float32 (created by layer 'concatenate_3')>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dists = [\n",
    "    TimeDistributed(Dense(N_STEPS)) for _ in range(11)\n",
    "]\n",
    "\n",
    "layers.concatenate([layer(concatenated_tensor)[..., None] for layer in time_dists])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 14, 7) dtype=float32 (created by layer 'time_distributed')>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay(concatenated_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " embed (InputLayer)             [(None, 14, 2)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 14, 2, 4)     848         ['embed[0][0]']                  \n",
      "                                                                                                  \n",
      " encoded (InputLayer)           [(None, 14, 30)]     0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 14, 8)        0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 38)       0           ['encoded[0][0]',                \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 14, 38)      76          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 14, 64)       26368       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 14, 64)      128         ['lstm[0][0]']                   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 14, 32)       12416       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 14, 32)      64          ['lstm_1[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 112)          64960       ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 112)         224         ['lstm_2[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 7, 16)        0           ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " reg (Dense)                    (None, 7, 1)         17          ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " clf (Dense)                    (None, 7, 11)        187         ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 105,288\n",
      "Trainable params: 105,288\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Reshape, LayerNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "model_comment = \"simple_LSTM\"\n",
    "\n",
    "concatenated_tensor = LayerNormalization()(concatenated_tensor)\n",
    "# Shared LSTM layer for both regression and classification\n",
    "_lstm_output = LSTM(64, kernel_regularizer=\"l1\", dropout=0.1, return_sequences=True)(concatenated_tensor)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "_lstm_output = LSTM(32, kernel_regularizer=\"l1\", dropout=0.1, return_sequences=True)(_lstm_output)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "_lstm_output = LSTM(16 * N_STEPS, kernel_regularizer=\"l1\", dropout=0.1)(_lstm_output)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "lstm_output = Reshape((N_STEPS, 16))(_lstm_output)\n",
    "\n",
    "# Regression output\n",
    "reg_output = Dense(1, activation='linear', name='reg')(lstm_output)\n",
    "\n",
    "# Classification output\n",
    "clf_output = Dense(11, activation='softmax', name=\"clf\")(lstm_output)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model = Model(inputs=[encoded, fare_basis], outputs=[reg_output, clf_output])\n",
    "\n",
    "# Compile the model with appropriate loss functions and metrics\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss={'reg': 'mse', 'clf': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'reg': 'mae', 'clf': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNormLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTMCell, LayerNormalization, RNN\n",
    "from keras import backend\n",
    "import tensorflow as tf\n",
    "\n",
    "class LayerNormLSTMCell(LSTMCell):\n",
    "    \"\"\"\n",
    "    LayerNormalization when cell return output and states.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(units, activation, **kwargs)\n",
    "        self.kernel_norm = LayerNormalization(name=\"kernel_norm\")\n",
    "        self.recurrent_norm = LayerNormalization(name=\"recurrent_norm\")\n",
    "        self.state_norm = LayerNormalization(name=\"state_norm\")\n",
    "    \n",
    "    def call(self, inputs, states, training=None):\n",
    "        h_tm1 = states[0]  # previous memory state\n",
    "        c_tm1 = states[1]  # previous carry state\n",
    "\n",
    "        dp_mask = self.get_dropout_mask_for_cell(inputs, training, count=4)\n",
    "        rec_dp_mask = self.get_recurrent_dropout_mask_for_cell(h_tm1, training, count=4)\n",
    "        if 0.0 < self.dropout < 1.0:\n",
    "            inputs *= dp_mask[0]\n",
    "        z = self.kernel_norm(backend.dot(inputs, self.kernel))\n",
    "\n",
    "        if 0.0 < self.recurrent_dropout < 1.0:\n",
    "            h_tm1 *= rec_dp_mask[0]\n",
    "        z += self.recurrent_norm(backend.dot(h_tm1, self.recurrent_kernel))\n",
    "        if self.use_bias:\n",
    "            z = backend.bias_add(z, self.bias)\n",
    "\n",
    "        z = tf.split(z, num_or_size_splits=4, axis=1)\n",
    "        c, o = self._compute_carry_and_output_fused(z, c_tm1)\n",
    "        c = self.state_norm(c)\n",
    "        h = o * self.activation(c)\n",
    "        return h, [h, c]\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "\n",
    "        def maybe_build_sublayer(sublayer, build_shape):\n",
    "            if not sublayer.built:\n",
    "                with tf.keras.backend.name_scope(sublayer.name):\n",
    "                    sublayer.build(build_shape)\n",
    "                    sublayer.built = True\n",
    "\n",
    "        maybe_build_sublayer(self.kernel_norm, [input_shape[0], self.units * 4])\n",
    "        maybe_build_sublayer(self.recurrent_norm, [input_shape[0], self.units * 4])\n",
    "        maybe_build_sublayer(self.state_norm, [input_shape[0], self.units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " fare_basis (InputLayer)        [(None, 14, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 14, 1, 4)     1000        ['fare_basis[0][0]']             \n",
      "                                                                                                  \n",
      " encoded (InputLayer)           [(None, 14, 34)]     0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 14, 4)        0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 38)       0           ['encoded[0][0]',                \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " LN_LSTM (RNN)                  (None, 32)           9664        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 7, 32)        0           ['LN_LSTM[0][0]']                \n",
      "                                                                                                  \n",
      " LN_LSTM_2 (RNN)                (None, 7, 32)        8896        ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " reg (TimeDistributed)          (None, 7, 1)         33          ['LN_LSTM_2[0][0]']              \n",
      "                                                                                                  \n",
      " clf (TimeDistributed)          (None, 7, 11)        363         ['LN_LSTM_2[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 19,956\n",
      "Trainable params: 19,956\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, RepeatVector, Dense, TimeDistributed\n",
    "from keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "model_comment = \"LayerNormLSTM\"\n",
    "\n",
    "LNLSTM_tensor = RNN(LayerNormLSTMCell(32, dropout=0.1, recurrent_dropout=0.1), name=\"LN_LSTM\")(concatenated_tensor)\n",
    "_lstm_output = RepeatVector(N_STEPS)(LNLSTM_tensor)\n",
    "lstm_output = RNN(LayerNormLSTMCell(32, dropout=0.1, recurrent_dropout=0.1), name=\"LN_LSTM_2\", return_sequences=True)(_lstm_output)\n",
    "\n",
    "reg_output = TimeDistributed(Dense(1, activation='relu'), name='reg')(lstm_output)\n",
    "clf_output = TimeDistributed(Dense(11, activation='softmax'), name=\"clf\")(lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoded, fare_basis], outputs=[reg_output, clf_output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'reg': 'mse', 'clf': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'reg': 'mae'}\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTMCell, LayerNormalization, RNN\n",
    "\n",
    "class LayerNormLSTMCell(LSTMCell):\n",
    "    \"\"\"\n",
    "    LayerNormalization when cell return output and states.\n",
    "    \"\"\"\n",
    "    def __init__(self, units, activation=\"tanh\", **kwargs):\n",
    "        super().__init__(units, activation, **kwargs)\n",
    "        self.__layer_norm = LayerNormalization()\n",
    "    \n",
    "    def call(self, inputs, states):\n",
    "        outputs, new_states = super().call(inputs, states)\n",
    "        norm_outputs = self.activation(self.__layer_norm(outputs))\n",
    "        norm_states = self.activation(self.__layer_norm(new_states[1]))\n",
    "        return norm_outputs, [norm_outputs, norm_states] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " embed (InputLayer)             [(None, 14, 2)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 14, 2, 4)     236         ['embed[0][0]']                  \n",
      "                                                                                                  \n",
      " encoded (InputLayer)           [(None, 14, 30)]     0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 14, 8)        0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 38)       0           ['encoded[0][0]',                \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 14, 38)      76          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 14, 64)       26368       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 14, 64)      128         ['lstm[0][0]']                   \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " LN_LSTM (RNN)                  (None, 14, 32)       12480       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 14, 32)      64          ['LN_LSTM[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 112)          64960       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 112)         224         ['lstm_1[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " reshape_1 (Reshape)            (None, 7, 16)        0           ['layer_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " reg (Dense)                    (None, 7, 1)         17          ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      " clf (Dense)                    (None, 7, 11)        187         ['reshape_1[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 104,740\n",
      "Trainable params: 104,740\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Embedding, LSTM, Dense, Concatenate, Reshape, BatchNormalization, LayerNormalization\n",
    "from keras.models import Model\n",
    "\n",
    "model_comment = \"LayerNormLSTM\"\n",
    "\n",
    "encoded = Input(shape=(LAGS, 30), name=\"encoded\")\n",
    "fare_basis = Input(shape=(LAGS, embed_n_feat), name=\"embed\")\n",
    "\n",
    "# Embedding layer for the second input\n",
    "embedded_tensor2 = Embedding(input_dim=embed_n_classes, output_dim=embed_dim)(fare_basis)\n",
    "embedded_tensor2 = Reshape((LAGS, embed_n_feat * embed_dim))(embedded_tensor2)\n",
    "\n",
    "# Concatenate the embedded tensor with the first input tensor\n",
    "concatenated_tensor = Concatenate(axis=-1)([encoded, embedded_tensor2])\n",
    "concatenated_tensor = LayerNormalization()(concatenated_tensor)\n",
    "\n",
    "# Shared LSTM layer for both regression and classification\n",
    "_lstm_output = LSTM(64, kernel_regularizer=\"l1\", dropout=0.1, return_sequences=True)(concatenated_tensor)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "# _lstm_output = LSTM(32, kernel_regularizer=\"l1\", dropout=0.1, return_sequences=True)(_lstm_output)\n",
    "_lstm_output = RNN(LayerNormLSTMCell(32, kernel_regularizer=\"l1\", dropout=0.1), return_sequences=True, name=\"LN_LSTM\")(_lstm_output)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "_lstm_output = LSTM(16 * N_STEPS, kernel_regularizer=\"l1\", dropout=0.1)(_lstm_output)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "lstm_output = Reshape((N_STEPS, 16))(_lstm_output)\n",
    "\n",
    "# Regression output\n",
    "reg_output = Dense(1, activation='linear', name='reg')(lstm_output)\n",
    "\n",
    "# Classification output\n",
    "clf_output = Dense(11, activation='softmax', name=\"clf\")(lstm_output)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model = Model(inputs=[encoded, fare_basis], outputs=[reg_output, clf_output])\n",
    "\n",
    "# Compile the model with appropriate loss functions and metrics\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss={'reg': 'mse', 'clf': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'reg': 'mae', 'clf': 'accuracy'}\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM encoder-decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " fare_basis (InputLayer)        [(None, 14, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 14, 1, 16)    4000        ['fare_basis[0][0]']             \n",
      "                                                                                                  \n",
      " encoded (InputLayer)           [(None, 14, 34)]     0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 14, 16)       0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 50)       0           ['encoded[0][0]',                \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    (None, 14, 50)       20200       ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " add (Add)                      (None, 14, 50)       0           ['lstm[0][0]',                   \n",
      "                                                                  'concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 14, 50)      100         ['add[0][0]']                    \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  (None, 14, 50)       20200       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " add_1 (Add)                    (None, 14, 50)       0           ['lstm_1[0][0]',                 \n",
      "                                                                  'layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 14, 50)      100         ['add_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " lstm_2 (LSTM)                  (None, 50)           20200       ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 50)          100         ['lstm_2[0][0]']                 \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 7, 50)        0           ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " lstm_3 (LSTM)                  (None, 7, 50)        20200       ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " add_2 (Add)                    (None, 7, 50)        0           ['lstm_3[0][0]',                 \n",
      "                                                                  'repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " layer_normalization_3 (LayerNo  (None, 7, 50)       100         ['add_2[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)                  (None, 7, 50)        20200       ['layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " add_3 (Add)                    (None, 7, 50)        0           ['lstm_4[0][0]',                 \n",
      "                                                                  'layer_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_4 (LayerNo  (None, 7, 50)       100         ['add_3[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 7, 16)       816         ['layer_normalization_4[0][0]']  \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      " reg (TimeDistributed)          (None, 7, 1)         17          ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      " clf (TimeDistributed)          (None, 7, 11)        187         ['time_distributed[0][0]']       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 106,520\n",
      "Trainable params: 106,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM, Dense, Concatenate, Reshape, RepeatVector, LayerNormalization, TimeDistributed, add\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.models import Model\n",
    "\n",
    "# model_comment = \"LSTM_encoder_decoder\"\n",
    "model_comment = \"embed_dim_test\"\n",
    "\n",
    "unit = concatenated_tensor.shape[-1]\n",
    "\n",
    "_lstm_output = LayerNormalization()(\n",
    "    add([LSTM(unit, kernel_regularizer=\"l2\", dropout=0.1, return_sequences=True)(concatenated_tensor), concatenated_tensor])\n",
    ")\n",
    "_lstm_output = LayerNormalization()(\n",
    "    add([LSTM(unit, kernel_regularizer=\"l2\", dropout=0.1, return_sequences=True)(_lstm_output), _lstm_output])\n",
    ")\n",
    "_lstm_output = LayerNormalization()(\n",
    "    LSTM(unit, kernel_regularizer=\"l2\", dropout=0.1)(_lstm_output)\n",
    ")\n",
    "lstm_encode = RepeatVector(N_STEPS)(_lstm_output)\n",
    "\n",
    "_lstm_decode = LayerNormalization()(\n",
    "    add([LSTM(unit, kernel_regularizer=\"l2\", dropout=0.1, return_sequences=True)(lstm_encode), lstm_encode])\n",
    ")\n",
    "lstm_output = LayerNormalization()(\n",
    "    add([LSTM(unit, kernel_regularizer=\"l2\", dropout=0.1, return_sequences=True)(_lstm_decode), _lstm_decode])\n",
    ")\n",
    "\n",
    "# Regression output\n",
    "lstm_output = TimeDistributed(\n",
    "    Dense(16, activation=\"relu\")\n",
    ")(lstm_output)\n",
    "reg_output = TimeDistributed(\n",
    "    Dense(1, activation='linear'), name='reg'\n",
    ")(lstm_output)\n",
    "\n",
    "# Classification output\n",
    "clf_output = TimeDistributed(\n",
    "    Dense(11, activation='softmax'), name=\"clf\"\n",
    ")(lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoded, fare_basis], outputs=[reg_output, clf_output])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss={'reg': 'mse', 'clf': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'reg': 'mae'}\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "tensor_board = TensorBoard(log_dir=f\"log/{model_comment}/lstm_16/\", histogram_freq=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " fare_basis (InputLayer)        [(None, 14, 1)]      0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 14, 1, 4)     1000        ['fare_basis[0][0]']             \n",
      "                                                                                                  \n",
      " encoded (InputLayer)           [(None, 14, 34)]     0           []                               \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 14, 4)        0           ['embedding[0][0]']              \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 14, 38)       0           ['encoded[0][0]',                \n",
      "                                                                  'reshape[0][0]']                \n",
      "                                                                                                  \n",
      " layer_normalization (LayerNorm  (None, 14, 38)      76          ['concatenate[0][0]']            \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " gru (GRU)                      (None, 14, 64)       19968       ['layer_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " layer_normalization_1 (LayerNo  (None, 14, 64)      128         ['gru[0][0]']                    \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " gru_1 (GRU)                    (None, 14, 32)       9408        ['layer_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " layer_normalization_2 (LayerNo  (None, 14, 32)      64          ['gru_1[0][0]']                  \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " gru_2 (GRU)                    (None, 16)           2400        ['layer_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " repeat_vector (RepeatVector)   (None, 7, 16)        0           ['gru_2[0][0]']                  \n",
      "                                                                                                  \n",
      " hidden_reg (Dense)             (None, 7, 8)         136         ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      " reg (Dense)                    (None, 7, 1)         9           ['hidden_reg[0][0]']             \n",
      "                                                                                                  \n",
      " clf (Dense)                    (None, 7, 11)        187         ['repeat_vector[0][0]']          \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 33,376\n",
      "Trainable params: 33,376\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "from keras.backend import clear_session\n",
    "from keras.layers import (GRU, Concatenate, Dense, Embedding, Input,\n",
    "                          LayerNormalization, RepeatVector, Reshape)\n",
    "from keras.models import Model\n",
    "\n",
    "from snapshot import *\n",
    "\n",
    "# window = joblib.load(\"WindowData_window.gz\")\n",
    "\n",
    "\n",
    "clear_session()\n",
    "\n",
    "embed_dim = 4\n",
    "\n",
    "encoded = Input(shape=(LAGS, 34), name=\"encoded\")\n",
    "fare_basis = Input(shape=(LAGS, 1), name=\"fare_basis\")\n",
    "\n",
    "embedded = Embedding(input_dim=250, output_dim=embed_dim)(fare_basis)\n",
    "embedded = Reshape((LAGS, embed_dim))(embedded)\n",
    "concatenated_tensor = Concatenate(axis=-1)([encoded, embedded])\n",
    "\n",
    "\n",
    "model_comment = \"GRU_embed_4\"\n",
    "\n",
    "concatenated_tensor = LayerNormalization()(concatenated_tensor)\n",
    "# Shared LSTM layer for both regression and classification\n",
    "_lstm_output = GRU(64, kernel_regularizer=\"l2\", dropout=0.1, return_sequences=True)(concatenated_tensor)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "_lstm_output = GRU(32, kernel_regularizer=\"l2\", dropout=0.1, return_sequences=True)(_lstm_output)\n",
    "_lstm_output = LayerNormalization()(_lstm_output)\n",
    "_lstm_output = GRU(16, kernel_regularizer=\"l2\", dropout=0.1)(_lstm_output)\n",
    "lstm_output = RepeatVector(N_STEPS)(_lstm_output)\n",
    "\n",
    "# Regression output\n",
    "_reg_output = Dense(8, activation=\"relu\", name=\"hidden_reg\")(lstm_output)\n",
    "reg_output = Dense(1, activation='linear', name='reg')(_reg_output)\n",
    "\n",
    "# Classification output\n",
    "clf_output = Dense(11, activation='softmax', name=\"clf\")(lstm_output)\n",
    "\n",
    "# Define the model with two outputs\n",
    "model = Model(inputs=[encoded, fare_basis], outputs=[reg_output, clf_output])\n",
    "\n",
    "# Compile the model with appropriate loss functions and metrics\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss={'reg': 'mse', 'clf': 'sparse_categorical_crossentropy'},\n",
    "    metrics={'reg': 'mae'}\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 7, 1) dtype=float32 (created by layer 'reg')>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class CausalModel(tf.keras.Model):\n",
    "    def __init__(self, lags=14, n_steps=7):\n",
    "        super(CausalModel, self).__init__()\n",
    "        # self.input = tf.keras.layers.Input([lags, 20])\n",
    "        self.time_dist_dense = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(n_steps))\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.time_dist_dense(inputs)\n",
    "\n",
    "    def predict(self, x):\n",
    "        pred = super(CausalModel, self).predict(x)\n",
    "        return pred[:, -1]\n",
    "\n",
    "model = CausalModel()\n",
    "\n",
    "# input_tensor = tf.keras.layers.Input([14, 20])\n",
    "# model(input_tensor)\n",
    "# model.save(\"custom\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
      "Model: \"causal_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_34 (TimeDi  multiple                 147       \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 147\n",
      "Trainable params: 147\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "_model = tf.keras.models.load_model(\"custom\")\n",
    "_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.ones((1, 14, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv1D\n",
    "\n",
    "\n",
    "conv = Conv1D(2, 3, padding=\"causal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_time_step_mse(Y_true, Y_pred):\n",
    "    return keras.metrics.mean_squared_error(Y_true[:, -1], Y_pred[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 14, 2), dtype=float32, numpy=\n",
       "array([[[-1.0523348 , -0.9404941 ],\n",
       "        [-0.6090739 , -1.1133223 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ],\n",
       "        [-0.24222544, -1.2826777 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded = np.concatenate([np.zeros((1, 1, 2)), arr, np.zeros((1, 1, 2))], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 6, 2), dtype=float32, numpy=\n",
       "array([[[-0.3246593,  0.4348855],\n",
       "        [-0.3246593,  0.4348855],\n",
       "        [-0.3246593,  0.4348855],\n",
       "        [-0.3246593,  0.4348855],\n",
       "        [-0.3246593,  0.4348855],\n",
       "        [-0.3246593,  0.4348855]]], dtype=float32)>"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_ = Conv1D(4, 3)\n",
    "conv_(padded);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_.set_weights(conv.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 14, 4), dtype=float32, numpy=\n",
       "array([[[ 0.34678614, -0.05552459,  0.84444267,  0.25828576],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.3067615 ,  0.24568313,  1.3583382 , -0.1781897 ],\n",
       "        [ 0.43274322,  0.5942487 ,  0.95666677,  0.34926495]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_run_data = joblib.load(\"test_run_data.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "38/38 [==============================] - 15s 122ms/step - loss: 5.0796 - reg_loss: 0.2700 - clf_loss: 1.3602 - reg_mae: 0.4327 - val_loss: 4.0346 - val_reg_loss: 0.2380 - val_clf_loss: 0.9640 - val_reg_mae: 0.4034\n",
      "Epoch 2/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 3.0058 - reg_loss: 0.1258 - clf_loss: 0.5309 - reg_mae: 0.3016 - val_loss: 2.7840 - val_reg_loss: 0.1693 - val_clf_loss: 0.7292 - val_reg_mae: 0.3304\n",
      "Epoch 3/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 2.0909 - reg_loss: 0.0949 - clf_loss: 0.4322 - reg_mae: 0.2599 - val_loss: 2.0476 - val_reg_loss: 0.1417 - val_clf_loss: 0.6394 - val_reg_mae: 0.3004\n",
      "Epoch 4/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 1.5329 - reg_loss: 0.0822 - clf_loss: 0.3828 - reg_mae: 0.2436 - val_loss: 1.5943 - val_reg_loss: 0.1308 - val_clf_loss: 0.5790 - val_reg_mae: 0.2853\n",
      "Epoch 5/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 1.1894 - reg_loss: 0.0726 - clf_loss: 0.3557 - reg_mae: 0.2282 - val_loss: 1.2899 - val_reg_loss: 0.1027 - val_clf_loss: 0.5414 - val_reg_mae: 0.2308\n",
      "Epoch 6/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.9700 - reg_loss: 0.0679 - clf_loss: 0.3358 - reg_mae: 0.2192 - val_loss: 1.1104 - val_reg_loss: 0.0999 - val_clf_loss: 0.5198 - val_reg_mae: 0.2280\n",
      "Epoch 7/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.8180 - reg_loss: 0.0619 - clf_loss: 0.3188 - reg_mae: 0.2094 - val_loss: 0.9795 - val_reg_loss: 0.0960 - val_clf_loss: 0.4978 - val_reg_mae: 0.2207\n",
      "Epoch 8/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.7149 - reg_loss: 0.0574 - clf_loss: 0.3101 - reg_mae: 0.2019 - val_loss: 0.8937 - val_reg_loss: 0.0983 - val_clf_loss: 0.4852 - val_reg_mae: 0.2267\n",
      "Epoch 9/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.6395 - reg_loss: 0.0564 - clf_loss: 0.3005 - reg_mae: 0.1999 - val_loss: 0.8254 - val_reg_loss: 0.1019 - val_clf_loss: 0.4682 - val_reg_mae: 0.2310\n",
      "Epoch 10/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.5828 - reg_loss: 0.0554 - clf_loss: 0.2931 - reg_mae: 0.1982 - val_loss: 0.7704 - val_reg_loss: 0.0943 - val_clf_loss: 0.4623 - val_reg_mae: 0.2201\n",
      "Epoch 11/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.5370 - reg_loss: 0.0538 - clf_loss: 0.2855 - reg_mae: 0.1930 - val_loss: 0.7150 - val_reg_loss: 0.0857 - val_clf_loss: 0.4477 - val_reg_mae: 0.2099\n",
      "Epoch 12/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.5025 - reg_loss: 0.0515 - clf_loss: 0.2822 - reg_mae: 0.1909 - val_loss: 0.6942 - val_reg_loss: 0.0905 - val_clf_loss: 0.4478 - val_reg_mae: 0.2170\n",
      "Epoch 13/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.4788 - reg_loss: 0.0510 - clf_loss: 0.2821 - reg_mae: 0.1880 - val_loss: 0.6617 - val_reg_loss: 0.0830 - val_clf_loss: 0.4430 - val_reg_mae: 0.2078\n",
      "Epoch 14/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.4593 - reg_loss: 0.0543 - clf_loss: 0.2774 - reg_mae: 0.1935 - val_loss: 0.6614 - val_reg_loss: 0.1013 - val_clf_loss: 0.4403 - val_reg_mae: 0.2296\n",
      "Epoch 15/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.4391 - reg_loss: 0.0525 - clf_loss: 0.2736 - reg_mae: 0.1890 - val_loss: 0.6212 - val_reg_loss: 0.0810 - val_clf_loss: 0.4340 - val_reg_mae: 0.2038\n",
      "Epoch 16/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.4201 - reg_loss: 0.0486 - clf_loss: 0.2708 - reg_mae: 0.1824 - val_loss: 0.6112 - val_reg_loss: 0.0839 - val_clf_loss: 0.4318 - val_reg_mae: 0.2027\n",
      "Epoch 17/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.4040 - reg_loss: 0.0465 - clf_loss: 0.2666 - reg_mae: 0.1799 - val_loss: 0.6016 - val_reg_loss: 0.0900 - val_clf_loss: 0.4253 - val_reg_mae: 0.2138\n",
      "Epoch 18/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3940 - reg_loss: 0.0481 - clf_loss: 0.2634 - reg_mae: 0.1824 - val_loss: 0.5991 - val_reg_loss: 0.0937 - val_clf_loss: 0.4267 - val_reg_mae: 0.2167\n",
      "Epoch 19/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3812 - reg_loss: 0.0479 - clf_loss: 0.2577 - reg_mae: 0.1802 - val_loss: 0.5804 - val_reg_loss: 0.0871 - val_clf_loss: 0.4208 - val_reg_mae: 0.2037\n",
      "Epoch 20/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3796 - reg_loss: 0.0505 - clf_loss: 0.2591 - reg_mae: 0.1854 - val_loss: 0.5927 - val_reg_loss: 0.0986 - val_clf_loss: 0.4265 - val_reg_mae: 0.2217\n",
      "Epoch 21/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3660 - reg_loss: 0.0474 - clf_loss: 0.2531 - reg_mae: 0.1809 - val_loss: 0.5762 - val_reg_loss: 0.0881 - val_clf_loss: 0.4251 - val_reg_mae: 0.2149\n",
      "Epoch 22/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3585 - reg_loss: 0.0456 - clf_loss: 0.2519 - reg_mae: 0.1767 - val_loss: 0.5644 - val_reg_loss: 0.0866 - val_clf_loss: 0.4189 - val_reg_mae: 0.2105\n",
      "Epoch 23/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3518 - reg_loss: 0.0448 - clf_loss: 0.2499 - reg_mae: 0.1752 - val_loss: 0.5540 - val_reg_loss: 0.0784 - val_clf_loss: 0.4203 - val_reg_mae: 0.1945\n",
      "Epoch 24/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3467 - reg_loss: 0.0466 - clf_loss: 0.2462 - reg_mae: 0.1774 - val_loss: 0.5622 - val_reg_loss: 0.0856 - val_clf_loss: 0.4239 - val_reg_mae: 0.2183\n",
      "Epoch 25/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3509 - reg_loss: 0.0474 - clf_loss: 0.2519 - reg_mae: 0.1791 - val_loss: 0.5581 - val_reg_loss: 0.0816 - val_clf_loss: 0.4260 - val_reg_mae: 0.2020\n",
      "Epoch 26/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3412 - reg_loss: 0.0460 - clf_loss: 0.2459 - reg_mae: 0.1740 - val_loss: 0.5438 - val_reg_loss: 0.0875 - val_clf_loss: 0.4082 - val_reg_mae: 0.2037\n",
      "Epoch 27/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3398 - reg_loss: 0.0461 - clf_loss: 0.2464 - reg_mae: 0.1747 - val_loss: 0.5680 - val_reg_loss: 0.0871 - val_clf_loss: 0.4346 - val_reg_mae: 0.2060\n",
      "Epoch 28/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3382 - reg_loss: 0.0456 - clf_loss: 0.2471 - reg_mae: 0.1753 - val_loss: 0.5322 - val_reg_loss: 0.0789 - val_clf_loss: 0.4086 - val_reg_mae: 0.1911\n",
      "Epoch 29/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3332 - reg_loss: 0.0435 - clf_loss: 0.2454 - reg_mae: 0.1703 - val_loss: 0.5415 - val_reg_loss: 0.0879 - val_clf_loss: 0.4099 - val_reg_mae: 0.2058\n",
      "Epoch 30/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3370 - reg_loss: 0.0463 - clf_loss: 0.2474 - reg_mae: 0.1793 - val_loss: 0.5390 - val_reg_loss: 0.0858 - val_clf_loss: 0.4104 - val_reg_mae: 0.2050\n",
      "Epoch 31/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3252 - reg_loss: 0.0432 - clf_loss: 0.2402 - reg_mae: 0.1692 - val_loss: 0.5502 - val_reg_loss: 0.1017 - val_clf_loss: 0.4075 - val_reg_mae: 0.2206\n",
      "Epoch 32/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3197 - reg_loss: 0.0414 - clf_loss: 0.2380 - reg_mae: 0.1657 - val_loss: 0.5239 - val_reg_loss: 0.0899 - val_clf_loss: 0.3943 - val_reg_mae: 0.2113\n",
      "Epoch 33/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3136 - reg_loss: 0.0414 - clf_loss: 0.2332 - reg_mae: 0.1673 - val_loss: 0.5210 - val_reg_loss: 0.0828 - val_clf_loss: 0.3996 - val_reg_mae: 0.1948\n",
      "Epoch 34/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3123 - reg_loss: 0.0435 - clf_loss: 0.2307 - reg_mae: 0.1705 - val_loss: 0.5086 - val_reg_loss: 0.0778 - val_clf_loss: 0.3933 - val_reg_mae: 0.1940\n",
      "Epoch 35/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3084 - reg_loss: 0.0412 - clf_loss: 0.2301 - reg_mae: 0.1645 - val_loss: 0.5019 - val_reg_loss: 0.0785 - val_clf_loss: 0.3868 - val_reg_mae: 0.2021\n",
      "Epoch 36/3000\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.3114 - reg_loss: 0.0428 - clf_loss: 0.2322 - reg_mae: 0.1704 - val_loss: 0.5096 - val_reg_loss: 0.0781 - val_clf_loss: 0.3951 - val_reg_mae: 0.2001\n",
      "Epoch 37/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.3077 - reg_loss: 0.0414 - clf_loss: 0.2300 - reg_mae: 0.1663 - val_loss: 0.5155 - val_reg_loss: 0.0872 - val_clf_loss: 0.3924 - val_reg_mae: 0.2069\n",
      "Epoch 38/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.3028 - reg_loss: 0.0399 - clf_loss: 0.2272 - reg_mae: 0.1646 - val_loss: 0.5045 - val_reg_loss: 0.0793 - val_clf_loss: 0.3900 - val_reg_mae: 0.1973\n",
      "Epoch 39/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2973 - reg_loss: 0.0412 - clf_loss: 0.2213 - reg_mae: 0.1659 - val_loss: 0.5037 - val_reg_loss: 0.0825 - val_clf_loss: 0.3865 - val_reg_mae: 0.2036\n",
      "Epoch 40/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.2997 - reg_loss: 0.0410 - clf_loss: 0.2241 - reg_mae: 0.1651 - val_loss: 0.4987 - val_reg_loss: 0.0809 - val_clf_loss: 0.3831 - val_reg_mae: 0.1984\n",
      "Epoch 41/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.3019 - reg_loss: 0.0431 - clf_loss: 0.2242 - reg_mae: 0.1722 - val_loss: 0.5089 - val_reg_loss: 0.0954 - val_clf_loss: 0.3784 - val_reg_mae: 0.2223\n",
      "Epoch 42/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2980 - reg_loss: 0.0417 - clf_loss: 0.2214 - reg_mae: 0.1684 - val_loss: 0.4840 - val_reg_loss: 0.0760 - val_clf_loss: 0.3734 - val_reg_mae: 0.1899\n",
      "Epoch 43/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.3025 - reg_loss: 0.0442 - clf_loss: 0.2237 - reg_mae: 0.1761 - val_loss: 0.4809 - val_reg_loss: 0.0799 - val_clf_loss: 0.3667 - val_reg_mae: 0.1927\n",
      "Epoch 44/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2900 - reg_loss: 0.0414 - clf_loss: 0.2148 - reg_mae: 0.1694 - val_loss: 0.4793 - val_reg_loss: 0.0706 - val_clf_loss: 0.3749 - val_reg_mae: 0.1812\n",
      "Epoch 45/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2849 - reg_loss: 0.0396 - clf_loss: 0.2121 - reg_mae: 0.1631 - val_loss: 0.4679 - val_reg_loss: 0.0716 - val_clf_loss: 0.3633 - val_reg_mae: 0.1820\n",
      "Epoch 46/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2940 - reg_loss: 0.0431 - clf_loss: 0.2179 - reg_mae: 0.1705 - val_loss: 0.4831 - val_reg_loss: 0.0863 - val_clf_loss: 0.3633 - val_reg_mae: 0.2026\n",
      "Epoch 47/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2912 - reg_loss: 0.0425 - clf_loss: 0.2153 - reg_mae: 0.1695 - val_loss: 0.4720 - val_reg_loss: 0.0795 - val_clf_loss: 0.3591 - val_reg_mae: 0.1946\n",
      "Epoch 48/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2866 - reg_loss: 0.0428 - clf_loss: 0.2108 - reg_mae: 0.1732 - val_loss: 0.4794 - val_reg_loss: 0.0809 - val_clf_loss: 0.3655 - val_reg_mae: 0.1942\n",
      "Epoch 49/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2801 - reg_loss: 0.0410 - clf_loss: 0.2067 - reg_mae: 0.1687 - val_loss: 0.4579 - val_reg_loss: 0.0664 - val_clf_loss: 0.3591 - val_reg_mae: 0.1749\n",
      "Epoch 50/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2743 - reg_loss: 0.0399 - clf_loss: 0.2025 - reg_mae: 0.1670 - val_loss: 0.4670 - val_reg_loss: 0.0727 - val_clf_loss: 0.3623 - val_reg_mae: 0.1854\n",
      "Epoch 51/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2775 - reg_loss: 0.0408 - clf_loss: 0.2045 - reg_mae: 0.1676 - val_loss: 0.4643 - val_reg_loss: 0.0699 - val_clf_loss: 0.3618 - val_reg_mae: 0.1769\n",
      "Epoch 52/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2776 - reg_loss: 0.0403 - clf_loss: 0.2049 - reg_mae: 0.1632 - val_loss: 0.4594 - val_reg_loss: 0.0792 - val_clf_loss: 0.3472 - val_reg_mae: 0.1930\n",
      "Epoch 53/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2814 - reg_loss: 0.0442 - clf_loss: 0.2046 - reg_mae: 0.1759 - val_loss: 0.4629 - val_reg_loss: 0.0721 - val_clf_loss: 0.3582 - val_reg_mae: 0.1817\n",
      "Epoch 54/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2784 - reg_loss: 0.0406 - clf_loss: 0.2055 - reg_mae: 0.1676 - val_loss: 0.4865 - val_reg_loss: 0.0952 - val_clf_loss: 0.3586 - val_reg_mae: 0.2119\n",
      "Epoch 55/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2767 - reg_loss: 0.0427 - clf_loss: 0.2010 - reg_mae: 0.1702 - val_loss: 0.4749 - val_reg_loss: 0.0932 - val_clf_loss: 0.3487 - val_reg_mae: 0.2120\n",
      "Epoch 56/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2708 - reg_loss: 0.0383 - clf_loss: 0.2001 - reg_mae: 0.1606 - val_loss: 0.4977 - val_reg_loss: 0.1084 - val_clf_loss: 0.3568 - val_reg_mae: 0.2288\n",
      "Epoch 57/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2709 - reg_loss: 0.0377 - clf_loss: 0.2010 - reg_mae: 0.1601 - val_loss: 0.4897 - val_reg_loss: 0.0981 - val_clf_loss: 0.3590 - val_reg_mae: 0.2375\n",
      "Epoch 58/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.2706 - reg_loss: 0.0420 - clf_loss: 0.1965 - reg_mae: 0.1701 - val_loss: 0.4922 - val_reg_loss: 0.1090 - val_clf_loss: 0.3511 - val_reg_mae: 0.2570\n",
      "Epoch 59/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.2684 - reg_loss: 0.0431 - clf_loss: 0.1939 - reg_mae: 0.1706 - val_loss: 0.4921 - val_reg_loss: 0.1077 - val_clf_loss: 0.3525 - val_reg_mae: 0.2447\n",
      "Epoch 60/3000\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.2714 - reg_loss: 0.0412 - clf_loss: 0.1984 - reg_mae: 0.1663 - val_loss: 0.4900 - val_reg_loss: 0.0970 - val_clf_loss: 0.3603 - val_reg_mae: 0.2401\n",
      "Epoch 61/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2676 - reg_loss: 0.0405 - clf_loss: 0.1942 - reg_mae: 0.1655 - val_loss: 0.4816 - val_reg_loss: 0.0957 - val_clf_loss: 0.3531 - val_reg_mae: 0.2276\n",
      "Epoch 62/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2588 - reg_loss: 0.0380 - clf_loss: 0.1889 - reg_mae: 0.1588 - val_loss: 0.4767 - val_reg_loss: 0.0978 - val_clf_loss: 0.3470 - val_reg_mae: 0.2343\n",
      "Epoch 63/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2617 - reg_loss: 0.0399 - clf_loss: 0.1904 - reg_mae: 0.1665 - val_loss: 0.4572 - val_reg_loss: 0.0771 - val_clf_loss: 0.3483 - val_reg_mae: 0.1982\n",
      "Epoch 64/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2572 - reg_loss: 0.0380 - clf_loss: 0.1878 - reg_mae: 0.1597 - val_loss: 0.4634 - val_reg_loss: 0.0799 - val_clf_loss: 0.3519 - val_reg_mae: 0.2039\n",
      "Epoch 65/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2540 - reg_loss: 0.0382 - clf_loss: 0.1844 - reg_mae: 0.1586 - val_loss: 0.4598 - val_reg_loss: 0.0823 - val_clf_loss: 0.3463 - val_reg_mae: 0.2150\n",
      "Epoch 66/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2537 - reg_loss: 0.0388 - clf_loss: 0.1840 - reg_mae: 0.1612 - val_loss: 0.4789 - val_reg_loss: 0.0855 - val_clf_loss: 0.3620 - val_reg_mae: 0.2186\n",
      "Epoch 67/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2461 - reg_loss: 0.0360 - clf_loss: 0.1794 - reg_mae: 0.1544 - val_loss: 0.4464 - val_reg_loss: 0.0711 - val_clf_loss: 0.3448 - val_reg_mae: 0.1895\n",
      "Epoch 68/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2437 - reg_loss: 0.0340 - clf_loss: 0.1799 - reg_mae: 0.1484 - val_loss: 0.4330 - val_reg_loss: 0.0700 - val_clf_loss: 0.3328 - val_reg_mae: 0.1842\n",
      "Epoch 69/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2556 - reg_loss: 0.0385 - clf_loss: 0.1870 - reg_mae: 0.1576 - val_loss: 0.4827 - val_reg_loss: 0.0694 - val_clf_loss: 0.3821 - val_reg_mae: 0.1975\n",
      "Epoch 70/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2633 - reg_loss: 0.0423 - clf_loss: 0.1884 - reg_mae: 0.1673 - val_loss: 0.4641 - val_reg_loss: 0.0796 - val_clf_loss: 0.3507 - val_reg_mae: 0.2096\n",
      "Epoch 71/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2642 - reg_loss: 0.0415 - clf_loss: 0.1887 - reg_mae: 0.1666 - val_loss: 0.4502 - val_reg_loss: 0.0760 - val_clf_loss: 0.3398 - val_reg_mae: 0.2065\n",
      "Epoch 72/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2498 - reg_loss: 0.0369 - clf_loss: 0.1793 - reg_mae: 0.1568 - val_loss: 0.4596 - val_reg_loss: 0.0713 - val_clf_loss: 0.3557 - val_reg_mae: 0.1839\n",
      "Epoch 73/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2383 - reg_loss: 0.0343 - clf_loss: 0.1723 - reg_mae: 0.1509 - val_loss: 0.4423 - val_reg_loss: 0.0755 - val_clf_loss: 0.3352 - val_reg_mae: 0.1944\n",
      "Epoch 74/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2373 - reg_loss: 0.0359 - clf_loss: 0.1706 - reg_mae: 0.1556 - val_loss: 0.4365 - val_reg_loss: 0.0695 - val_clf_loss: 0.3362 - val_reg_mae: 0.1730\n",
      "Epoch 75/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2354 - reg_loss: 0.0331 - clf_loss: 0.1718 - reg_mae: 0.1450 - val_loss: 0.4524 - val_reg_loss: 0.0752 - val_clf_loss: 0.3467 - val_reg_mae: 0.1927\n",
      "Epoch 76/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2333 - reg_loss: 0.0337 - clf_loss: 0.1698 - reg_mae: 0.1480 - val_loss: 0.4566 - val_reg_loss: 0.0684 - val_clf_loss: 0.3583 - val_reg_mae: 0.1834\n",
      "Epoch 77/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2411 - reg_loss: 0.0355 - clf_loss: 0.1756 - reg_mae: 0.1519 - val_loss: 0.4426 - val_reg_loss: 0.0719 - val_clf_loss: 0.3403 - val_reg_mae: 0.1930\n",
      "Epoch 78/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2336 - reg_loss: 0.0360 - clf_loss: 0.1673 - reg_mae: 0.1515 - val_loss: 0.4601 - val_reg_loss: 0.0708 - val_clf_loss: 0.3584 - val_reg_mae: 0.1893\n",
      "Epoch 79/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2351 - reg_loss: 0.0369 - clf_loss: 0.1677 - reg_mae: 0.1539 - val_loss: 0.4585 - val_reg_loss: 0.0827 - val_clf_loss: 0.3449 - val_reg_mae: 0.2096\n",
      "Epoch 80/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2421 - reg_loss: 0.0376 - clf_loss: 0.1734 - reg_mae: 0.1593 - val_loss: 0.4629 - val_reg_loss: 0.0912 - val_clf_loss: 0.3399 - val_reg_mae: 0.2194\n",
      "Epoch 81/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2404 - reg_loss: 0.0378 - clf_loss: 0.1704 - reg_mae: 0.1594 - val_loss: 0.4592 - val_reg_loss: 0.0709 - val_clf_loss: 0.3551 - val_reg_mae: 0.1817\n",
      "Epoch 82/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.2456 - reg_loss: 0.0359 - clf_loss: 0.1767 - reg_mae: 0.1533 - val_loss: 0.4499 - val_reg_loss: 0.0688 - val_clf_loss: 0.3483 - val_reg_mae: 0.1901\n",
      "Epoch 83/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2542 - reg_loss: 0.0362 - clf_loss: 0.1850 - reg_mae: 0.1560 - val_loss: 0.4907 - val_reg_loss: 0.0730 - val_clf_loss: 0.3829 - val_reg_mae: 0.1923\n",
      "Epoch 84/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2421 - reg_loss: 0.0367 - clf_loss: 0.1708 - reg_mae: 0.1533 - val_loss: 0.4436 - val_reg_loss: 0.0615 - val_clf_loss: 0.3479 - val_reg_mae: 0.1768\n",
      "Epoch 85/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2275 - reg_loss: 0.0339 - clf_loss: 0.1608 - reg_mae: 0.1489 - val_loss: 0.4348 - val_reg_loss: 0.0651 - val_clf_loss: 0.3373 - val_reg_mae: 0.1703\n",
      "Epoch 86/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2228 - reg_loss: 0.0320 - clf_loss: 0.1587 - reg_mae: 0.1425 - val_loss: 0.4491 - val_reg_loss: 0.0684 - val_clf_loss: 0.3486 - val_reg_mae: 0.1812\n",
      "Epoch 87/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2156 - reg_loss: 0.0316 - clf_loss: 0.1532 - reg_mae: 0.1430 - val_loss: 0.4331 - val_reg_loss: 0.0607 - val_clf_loss: 0.3418 - val_reg_mae: 0.1669\n",
      "Epoch 88/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2152 - reg_loss: 0.0305 - clf_loss: 0.1549 - reg_mae: 0.1402 - val_loss: 0.4418 - val_reg_loss: 0.0670 - val_clf_loss: 0.3450 - val_reg_mae: 0.1845\n",
      "Epoch 89/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2071 - reg_loss: 0.0299 - clf_loss: 0.1483 - reg_mae: 0.1368 - val_loss: 0.4186 - val_reg_loss: 0.0589 - val_clf_loss: 0.3309 - val_reg_mae: 0.1623\n",
      "Epoch 90/3000\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.2098 - reg_loss: 0.0305 - clf_loss: 0.1510 - reg_mae: 0.1392 - val_loss: 0.4285 - val_reg_loss: 0.0646 - val_clf_loss: 0.3351 - val_reg_mae: 0.1633\n",
      "Epoch 91/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.2152 - reg_loss: 0.0310 - clf_loss: 0.1558 - reg_mae: 0.1401 - val_loss: 0.4342 - val_reg_loss: 0.0677 - val_clf_loss: 0.3366 - val_reg_mae: 0.1755\n",
      "Epoch 92/3000\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.2229 - reg_loss: 0.0343 - clf_loss: 0.1584 - reg_mae: 0.1489 - val_loss: 0.4584 - val_reg_loss: 0.0793 - val_clf_loss: 0.3484 - val_reg_mae: 0.1898\n",
      "Epoch 93/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2247 - reg_loss: 0.0335 - clf_loss: 0.1605 - reg_mae: 0.1476 - val_loss: 0.4517 - val_reg_loss: 0.0708 - val_clf_loss: 0.3493 - val_reg_mae: 0.1813\n",
      "Epoch 94/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2418 - reg_loss: 0.0346 - clf_loss: 0.1735 - reg_mae: 0.1501 - val_loss: 0.4561 - val_reg_loss: 0.0778 - val_clf_loss: 0.3432 - val_reg_mae: 0.1834\n",
      "Epoch 95/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2239 - reg_loss: 0.0336 - clf_loss: 0.1559 - reg_mae: 0.1469 - val_loss: 0.4510 - val_reg_loss: 0.0716 - val_clf_loss: 0.3457 - val_reg_mae: 0.1749\n",
      "Epoch 96/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2194 - reg_loss: 0.0324 - clf_loss: 0.1542 - reg_mae: 0.1444 - val_loss: 0.4491 - val_reg_loss: 0.0682 - val_clf_loss: 0.3485 - val_reg_mae: 0.1733\n",
      "Epoch 97/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2088 - reg_loss: 0.0309 - clf_loss: 0.1467 - reg_mae: 0.1418 - val_loss: 0.4804 - val_reg_loss: 0.0781 - val_clf_loss: 0.3714 - val_reg_mae: 0.1960\n",
      "Epoch 98/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2098 - reg_loss: 0.0329 - clf_loss: 0.1469 - reg_mae: 0.1446 - val_loss: 0.4501 - val_reg_loss: 0.0613 - val_clf_loss: 0.3582 - val_reg_mae: 0.1785\n",
      "Epoch 99/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2125 - reg_loss: 0.0319 - clf_loss: 0.1504 - reg_mae: 0.1428 - val_loss: 0.4331 - val_reg_loss: 0.0670 - val_clf_loss: 0.3361 - val_reg_mae: 0.1837\n",
      "Epoch 100/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2055 - reg_loss: 0.0307 - clf_loss: 0.1454 - reg_mae: 0.1391 - val_loss: 0.4413 - val_reg_loss: 0.0736 - val_clf_loss: 0.3384 - val_reg_mae: 0.1854\n",
      "Epoch 101/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2031 - reg_loss: 0.0309 - clf_loss: 0.1434 - reg_mae: 0.1413 - val_loss: 0.4441 - val_reg_loss: 0.0643 - val_clf_loss: 0.3511 - val_reg_mae: 0.1845\n",
      "Epoch 102/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1991 - reg_loss: 0.0302 - clf_loss: 0.1407 - reg_mae: 0.1384 - val_loss: 0.4336 - val_reg_loss: 0.0679 - val_clf_loss: 0.3374 - val_reg_mae: 0.1640\n",
      "Epoch 103/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1944 - reg_loss: 0.0290 - clf_loss: 0.1377 - reg_mae: 0.1347 - val_loss: 0.4309 - val_reg_loss: 0.0613 - val_clf_loss: 0.3415 - val_reg_mae: 0.1631\n",
      "Epoch 104/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2058 - reg_loss: 0.0318 - clf_loss: 0.1458 - reg_mae: 0.1423 - val_loss: 0.4171 - val_reg_loss: 0.0602 - val_clf_loss: 0.3280 - val_reg_mae: 0.1620\n",
      "Epoch 105/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2072 - reg_loss: 0.0308 - clf_loss: 0.1472 - reg_mae: 0.1411 - val_loss: 0.4541 - val_reg_loss: 0.0681 - val_clf_loss: 0.3564 - val_reg_mae: 0.1875\n",
      "Epoch 106/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.2053 - reg_loss: 0.0323 - clf_loss: 0.1434 - reg_mae: 0.1436 - val_loss: 0.4327 - val_reg_loss: 0.0665 - val_clf_loss: 0.3362 - val_reg_mae: 0.1782\n",
      "Epoch 107/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1996 - reg_loss: 0.0305 - clf_loss: 0.1397 - reg_mae: 0.1403 - val_loss: 0.4361 - val_reg_loss: 0.0579 - val_clf_loss: 0.3492 - val_reg_mae: 0.1679\n",
      "Epoch 108/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1976 - reg_loss: 0.0300 - clf_loss: 0.1389 - reg_mae: 0.1382 - val_loss: 0.4377 - val_reg_loss: 0.0657 - val_clf_loss: 0.3431 - val_reg_mae: 0.1874\n",
      "Epoch 109/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1934 - reg_loss: 0.0300 - clf_loss: 0.1349 - reg_mae: 0.1391 - val_loss: 0.4284 - val_reg_loss: 0.0669 - val_clf_loss: 0.3330 - val_reg_mae: 0.1907\n",
      "Epoch 110/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2246 - reg_loss: 0.0359 - clf_loss: 0.1587 - reg_mae: 0.1533 - val_loss: 0.5068 - val_reg_loss: 0.0782 - val_clf_loss: 0.3953 - val_reg_mae: 0.2042\n",
      "Epoch 111/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2299 - reg_loss: 0.0369 - clf_loss: 0.1589 - reg_mae: 0.1567 - val_loss: 0.4345 - val_reg_loss: 0.0701 - val_clf_loss: 0.3308 - val_reg_mae: 0.1876\n",
      "Epoch 112/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2070 - reg_loss: 0.0330 - clf_loss: 0.1414 - reg_mae: 0.1453 - val_loss: 0.4347 - val_reg_loss: 0.0685 - val_clf_loss: 0.3340 - val_reg_mae: 0.1755\n",
      "Epoch 113/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2049 - reg_loss: 0.0308 - clf_loss: 0.1420 - reg_mae: 0.1410 - val_loss: 0.4263 - val_reg_loss: 0.0645 - val_clf_loss: 0.3296 - val_reg_mae: 0.1700\n",
      "Epoch 114/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2031 - reg_loss: 0.0314 - clf_loss: 0.1400 - reg_mae: 0.1403 - val_loss: 0.4285 - val_reg_loss: 0.0582 - val_clf_loss: 0.3387 - val_reg_mae: 0.1674\n",
      "Epoch 115/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2007 - reg_loss: 0.0306 - clf_loss: 0.1391 - reg_mae: 0.1421 - val_loss: 0.4191 - val_reg_loss: 0.0602 - val_clf_loss: 0.3280 - val_reg_mae: 0.1657\n",
      "Epoch 116/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1931 - reg_loss: 0.0292 - clf_loss: 0.1330 - reg_mae: 0.1354 - val_loss: 0.4430 - val_reg_loss: 0.0679 - val_clf_loss: 0.3443 - val_reg_mae: 0.1754\n",
      "Epoch 117/3000\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.1871 - reg_loss: 0.0288 - clf_loss: 0.1282 - reg_mae: 0.1351 - val_loss: 0.4212 - val_reg_loss: 0.0625 - val_clf_loss: 0.3288 - val_reg_mae: 0.1733\n",
      "Epoch 118/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1873 - reg_loss: 0.0301 - clf_loss: 0.1278 - reg_mae: 0.1366 - val_loss: 0.4213 - val_reg_loss: 0.0619 - val_clf_loss: 0.3298 - val_reg_mae: 0.1649\n",
      "Epoch 119/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1882 - reg_loss: 0.0285 - clf_loss: 0.1302 - reg_mae: 0.1361 - val_loss: 0.4226 - val_reg_loss: 0.0614 - val_clf_loss: 0.3316 - val_reg_mae: 0.1688\n",
      "Epoch 120/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1887 - reg_loss: 0.0285 - clf_loss: 0.1307 - reg_mae: 0.1349 - val_loss: 0.4418 - val_reg_loss: 0.0623 - val_clf_loss: 0.3499 - val_reg_mae: 0.1611\n",
      "Epoch 121/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1862 - reg_loss: 0.0289 - clf_loss: 0.1277 - reg_mae: 0.1351 - val_loss: 0.4467 - val_reg_loss: 0.0611 - val_clf_loss: 0.3557 - val_reg_mae: 0.1563\n",
      "Epoch 122/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1819 - reg_loss: 0.0291 - clf_loss: 0.1236 - reg_mae: 0.1360 - val_loss: 0.4480 - val_reg_loss: 0.0679 - val_clf_loss: 0.3510 - val_reg_mae: 0.1751\n",
      "Epoch 123/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1797 - reg_loss: 0.0279 - clf_loss: 0.1231 - reg_mae: 0.1334 - val_loss: 0.4122 - val_reg_loss: 0.0604 - val_clf_loss: 0.3233 - val_reg_mae: 0.1652\n",
      "Epoch 124/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1847 - reg_loss: 0.0278 - clf_loss: 0.1279 - reg_mae: 0.1332 - val_loss: 0.4191 - val_reg_loss: 0.0564 - val_clf_loss: 0.3333 - val_reg_mae: 0.1544\n",
      "Epoch 125/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1822 - reg_loss: 0.0282 - clf_loss: 0.1251 - reg_mae: 0.1338 - val_loss: 0.4124 - val_reg_loss: 0.0594 - val_clf_loss: 0.3241 - val_reg_mae: 0.1596\n",
      "Epoch 126/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1782 - reg_loss: 0.0267 - clf_loss: 0.1229 - reg_mae: 0.1310 - val_loss: 0.4215 - val_reg_loss: 0.0651 - val_clf_loss: 0.3277 - val_reg_mae: 0.1669\n",
      "Epoch 127/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1846 - reg_loss: 0.0308 - clf_loss: 0.1254 - reg_mae: 0.1377 - val_loss: 0.4222 - val_reg_loss: 0.0587 - val_clf_loss: 0.3341 - val_reg_mae: 0.1657\n",
      "Epoch 128/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2013 - reg_loss: 0.0340 - clf_loss: 0.1339 - reg_mae: 0.1465 - val_loss: 0.4103 - val_reg_loss: 0.0556 - val_clf_loss: 0.3207 - val_reg_mae: 0.1580\n",
      "Epoch 129/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1992 - reg_loss: 0.0319 - clf_loss: 0.1337 - reg_mae: 0.1421 - val_loss: 0.4122 - val_reg_loss: 0.0570 - val_clf_loss: 0.3212 - val_reg_mae: 0.1604\n",
      "Epoch 130/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1976 - reg_loss: 0.0307 - clf_loss: 0.1331 - reg_mae: 0.1386 - val_loss: 0.4061 - val_reg_loss: 0.0598 - val_clf_loss: 0.3131 - val_reg_mae: 0.1651\n",
      "Epoch 131/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1925 - reg_loss: 0.0309 - clf_loss: 0.1280 - reg_mae: 0.1398 - val_loss: 0.4266 - val_reg_loss: 0.0536 - val_clf_loss: 0.3378 - val_reg_mae: 0.1612\n",
      "Epoch 132/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2030 - reg_loss: 0.0288 - clf_loss: 0.1390 - reg_mae: 0.1365 - val_loss: 0.4278 - val_reg_loss: 0.0580 - val_clf_loss: 0.3347 - val_reg_mae: 0.1614\n",
      "Epoch 133/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2046 - reg_loss: 0.0330 - clf_loss: 0.1358 - reg_mae: 0.1430 - val_loss: 0.4147 - val_reg_loss: 0.0613 - val_clf_loss: 0.3174 - val_reg_mae: 0.1743\n",
      "Epoch 134/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1854 - reg_loss: 0.0281 - clf_loss: 0.1227 - reg_mae: 0.1346 - val_loss: 0.4314 - val_reg_loss: 0.0663 - val_clf_loss: 0.3316 - val_reg_mae: 0.1693\n",
      "Epoch 135/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1746 - reg_loss: 0.0258 - clf_loss: 0.1164 - reg_mae: 0.1269 - val_loss: 0.4039 - val_reg_loss: 0.0615 - val_clf_loss: 0.3103 - val_reg_mae: 0.1600\n",
      "Epoch 136/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1754 - reg_loss: 0.0251 - clf_loss: 0.1180 - reg_mae: 0.1241 - val_loss: 0.4226 - val_reg_loss: 0.0568 - val_clf_loss: 0.3336 - val_reg_mae: 0.1508\n",
      "Epoch 137/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1716 - reg_loss: 0.0266 - clf_loss: 0.1136 - reg_mae: 0.1302 - val_loss: 0.3984 - val_reg_loss: 0.0568 - val_clf_loss: 0.3111 - val_reg_mae: 0.1461\n",
      "Epoch 138/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1678 - reg_loss: 0.0256 - clf_loss: 0.1121 - reg_mae: 0.1266 - val_loss: 0.4091 - val_reg_loss: 0.0554 - val_clf_loss: 0.3240 - val_reg_mae: 0.1505\n",
      "Epoch 139/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1664 - reg_loss: 0.0266 - clf_loss: 0.1106 - reg_mae: 0.1283 - val_loss: 0.3983 - val_reg_loss: 0.0630 - val_clf_loss: 0.3064 - val_reg_mae: 0.1636\n",
      "Epoch 140/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1622 - reg_loss: 0.0251 - clf_loss: 0.1085 - reg_mae: 0.1242 - val_loss: 0.4175 - val_reg_loss: 0.0613 - val_clf_loss: 0.3277 - val_reg_mae: 0.1671\n",
      "Epoch 141/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1775 - reg_loss: 0.0276 - clf_loss: 0.1208 - reg_mae: 0.1344 - val_loss: 0.4730 - val_reg_loss: 0.0720 - val_clf_loss: 0.3703 - val_reg_mae: 0.1869\n",
      "Epoch 142/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1809 - reg_loss: 0.0277 - clf_loss: 0.1218 - reg_mae: 0.1325 - val_loss: 0.4213 - val_reg_loss: 0.0544 - val_clf_loss: 0.3345 - val_reg_mae: 0.1532\n",
      "Epoch 143/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1714 - reg_loss: 0.0264 - clf_loss: 0.1133 - reg_mae: 0.1297 - val_loss: 0.4009 - val_reg_loss: 0.0533 - val_clf_loss: 0.3160 - val_reg_mae: 0.1495\n",
      "Epoch 144/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1695 - reg_loss: 0.0269 - clf_loss: 0.1118 - reg_mae: 0.1314 - val_loss: 0.3947 - val_reg_loss: 0.0540 - val_clf_loss: 0.3102 - val_reg_mae: 0.1522\n",
      "Epoch 145/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1633 - reg_loss: 0.0262 - clf_loss: 0.1071 - reg_mae: 0.1282 - val_loss: 0.4165 - val_reg_loss: 0.0569 - val_clf_loss: 0.3297 - val_reg_mae: 0.1577\n",
      "Epoch 146/3000\n",
      "38/38 [==============================] - 2s 49ms/step - loss: 0.1680 - reg_loss: 0.0263 - clf_loss: 0.1123 - reg_mae: 0.1297 - val_loss: 0.3988 - val_reg_loss: 0.0598 - val_clf_loss: 0.3097 - val_reg_mae: 0.1683\n",
      "Epoch 147/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1581 - reg_loss: 0.0249 - clf_loss: 0.1044 - reg_mae: 0.1243 - val_loss: 0.3808 - val_reg_loss: 0.0554 - val_clf_loss: 0.2965 - val_reg_mae: 0.1542\n",
      "Epoch 148/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1558 - reg_loss: 0.0241 - clf_loss: 0.1031 - reg_mae: 0.1234 - val_loss: 0.4186 - val_reg_loss: 0.0587 - val_clf_loss: 0.3314 - val_reg_mae: 0.1533\n",
      "Epoch 149/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1599 - reg_loss: 0.0255 - clf_loss: 0.1057 - reg_mae: 0.1268 - val_loss: 0.4052 - val_reg_loss: 0.0530 - val_clf_loss: 0.3232 - val_reg_mae: 0.1496\n",
      "Epoch 150/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1585 - reg_loss: 0.0245 - clf_loss: 0.1049 - reg_mae: 0.1240 - val_loss: 0.4107 - val_reg_loss: 0.0568 - val_clf_loss: 0.3251 - val_reg_mae: 0.1554\n",
      "Epoch 151/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1601 - reg_loss: 0.0261 - clf_loss: 0.1055 - reg_mae: 0.1280 - val_loss: 0.3976 - val_reg_loss: 0.0577 - val_clf_loss: 0.3109 - val_reg_mae: 0.1629\n",
      "Epoch 152/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1647 - reg_loss: 0.0255 - clf_loss: 0.1095 - reg_mae: 0.1280 - val_loss: 0.4714 - val_reg_loss: 0.0646 - val_clf_loss: 0.3768 - val_reg_mae: 0.1718\n",
      "Epoch 153/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1635 - reg_loss: 0.0258 - clf_loss: 0.1081 - reg_mae: 0.1273 - val_loss: 0.4901 - val_reg_loss: 0.0718 - val_clf_loss: 0.3888 - val_reg_mae: 0.1733\n",
      "Epoch 154/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1593 - reg_loss: 0.0247 - clf_loss: 0.1053 - reg_mae: 0.1240 - val_loss: 0.4079 - val_reg_loss: 0.0574 - val_clf_loss: 0.3210 - val_reg_mae: 0.1567\n",
      "Epoch 155/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1669 - reg_loss: 0.0263 - clf_loss: 0.1104 - reg_mae: 0.1284 - val_loss: 0.4053 - val_reg_loss: 0.0575 - val_clf_loss: 0.3164 - val_reg_mae: 0.1631\n",
      "Epoch 156/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2017 - reg_loss: 0.0316 - clf_loss: 0.1355 - reg_mae: 0.1425 - val_loss: 0.5459 - val_reg_loss: 0.0644 - val_clf_loss: 0.4419 - val_reg_mae: 0.1684\n",
      "Epoch 157/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2517 - reg_loss: 0.0378 - clf_loss: 0.1714 - reg_mae: 0.1543 - val_loss: 0.4496 - val_reg_loss: 0.0678 - val_clf_loss: 0.3366 - val_reg_mae: 0.1917\n",
      "Epoch 158/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.2510 - reg_loss: 0.0372 - clf_loss: 0.1613 - reg_mae: 0.1552 - val_loss: 0.4359 - val_reg_loss: 0.0638 - val_clf_loss: 0.3211 - val_reg_mae: 0.1877\n",
      "Epoch 159/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.2053 - reg_loss: 0.0311 - clf_loss: 0.1264 - reg_mae: 0.1437 - val_loss: 0.4133 - val_reg_loss: 0.0624 - val_clf_loss: 0.3067 - val_reg_mae: 0.1793\n",
      "Epoch 160/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1848 - reg_loss: 0.0287 - clf_loss: 0.1142 - reg_mae: 0.1346 - val_loss: 0.4025 - val_reg_loss: 0.0589 - val_clf_loss: 0.3037 - val_reg_mae: 0.1632\n",
      "Epoch 161/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1830 - reg_loss: 0.0267 - clf_loss: 0.1157 - reg_mae: 0.1291 - val_loss: 0.4290 - val_reg_loss: 0.0549 - val_clf_loss: 0.3324 - val_reg_mae: 0.1685\n",
      "Epoch 162/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1773 - reg_loss: 0.0284 - clf_loss: 0.1084 - reg_mae: 0.1359 - val_loss: 0.3984 - val_reg_loss: 0.0542 - val_clf_loss: 0.3060 - val_reg_mae: 0.1554\n",
      "Epoch 163/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1676 - reg_loss: 0.0264 - clf_loss: 0.1038 - reg_mae: 0.1280 - val_loss: 0.3958 - val_reg_loss: 0.0540 - val_clf_loss: 0.3053 - val_reg_mae: 0.1481\n",
      "Epoch 164/3000\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1624 - reg_loss: 0.0254 - clf_loss: 0.1012 - reg_mae: 0.1245 - val_loss: 0.4027 - val_reg_loss: 0.0591 - val_clf_loss: 0.3084 - val_reg_mae: 0.1539\n",
      "Epoch 165/3000\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1558 - reg_loss: 0.0240 - clf_loss: 0.0977 - reg_mae: 0.1208 - val_loss: 0.3776 - val_reg_loss: 0.0499 - val_clf_loss: 0.2945 - val_reg_mae: 0.1408\n",
      "Epoch 166/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1610 - reg_loss: 0.0260 - clf_loss: 0.1006 - reg_mae: 0.1279 - val_loss: 0.4144 - val_reg_loss: 0.0593 - val_clf_loss: 0.3199 - val_reg_mae: 0.1542\n",
      "Epoch 167/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1654 - reg_loss: 0.0265 - clf_loss: 0.1039 - reg_mae: 0.1293 - val_loss: 0.4053 - val_reg_loss: 0.0576 - val_clf_loss: 0.3133 - val_reg_mae: 0.1491\n",
      "Epoch 168/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1539 - reg_loss: 0.0250 - clf_loss: 0.0951 - reg_mae: 0.1252 - val_loss: 0.3877 - val_reg_loss: 0.0543 - val_clf_loss: 0.3004 - val_reg_mae: 0.1439\n",
      "Epoch 169/3000\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1444 - reg_loss: 0.0237 - clf_loss: 0.0886 - reg_mae: 0.1221 - val_loss: 0.3857 - val_reg_loss: 0.0552 - val_clf_loss: 0.2992 - val_reg_mae: 0.1511\n",
      "Epoch 170/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1409 - reg_loss: 0.0231 - clf_loss: 0.0868 - reg_mae: 0.1189 - val_loss: 0.3906 - val_reg_loss: 0.0519 - val_clf_loss: 0.3082 - val_reg_mae: 0.1469\n",
      "Epoch 171/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1418 - reg_loss: 0.0225 - clf_loss: 0.0893 - reg_mae: 0.1178 - val_loss: 0.3872 - val_reg_loss: 0.0522 - val_clf_loss: 0.3052 - val_reg_mae: 0.1469\n",
      "Epoch 172/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1512 - reg_loss: 0.0254 - clf_loss: 0.0955 - reg_mae: 0.1229 - val_loss: 0.3886 - val_reg_loss: 0.0536 - val_clf_loss: 0.3048 - val_reg_mae: 0.1557\n",
      "Epoch 173/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1690 - reg_loss: 0.0277 - clf_loss: 0.1096 - reg_mae: 0.1327 - val_loss: 0.4022 - val_reg_loss: 0.0557 - val_clf_loss: 0.3135 - val_reg_mae: 0.1530\n",
      "Epoch 174/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1657 - reg_loss: 0.0269 - clf_loss: 0.1053 - reg_mae: 0.1306 - val_loss: 0.4007 - val_reg_loss: 0.0538 - val_clf_loss: 0.3138 - val_reg_mae: 0.1565\n",
      "Epoch 175/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1474 - reg_loss: 0.0253 - clf_loss: 0.0894 - reg_mae: 0.1251 - val_loss: 0.4089 - val_reg_loss: 0.0588 - val_clf_loss: 0.3184 - val_reg_mae: 0.1609\n",
      "Epoch 176/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1414 - reg_loss: 0.0231 - clf_loss: 0.0871 - reg_mae: 0.1202 - val_loss: 0.3964 - val_reg_loss: 0.0607 - val_clf_loss: 0.3051 - val_reg_mae: 0.1678\n",
      "Epoch 177/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1437 - reg_loss: 0.0233 - clf_loss: 0.0898 - reg_mae: 0.1212 - val_loss: 0.3954 - val_reg_loss: 0.0557 - val_clf_loss: 0.3091 - val_reg_mae: 0.1527\n",
      "Epoch 178/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1465 - reg_loss: 0.0235 - clf_loss: 0.0926 - reg_mae: 0.1204 - val_loss: 0.4100 - val_reg_loss: 0.0685 - val_clf_loss: 0.3112 - val_reg_mae: 0.1745\n",
      "Epoch 179/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1557 - reg_loss: 0.0250 - clf_loss: 0.1001 - reg_mae: 0.1266 - val_loss: 0.4093 - val_reg_loss: 0.0536 - val_clf_loss: 0.3241 - val_reg_mae: 0.1480\n",
      "Epoch 180/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1488 - reg_loss: 0.0243 - clf_loss: 0.0929 - reg_mae: 0.1231 - val_loss: 0.3900 - val_reg_loss: 0.0508 - val_clf_loss: 0.3082 - val_reg_mae: 0.1501\n",
      "Epoch 181/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1420 - reg_loss: 0.0239 - clf_loss: 0.0877 - reg_mae: 0.1217 - val_loss: 0.3890 - val_reg_loss: 0.0573 - val_clf_loss: 0.3017 - val_reg_mae: 0.1485\n",
      "Epoch 182/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1403 - reg_loss: 0.0230 - clf_loss: 0.0874 - reg_mae: 0.1192 - val_loss: 0.3762 - val_reg_loss: 0.0516 - val_clf_loss: 0.2949 - val_reg_mae: 0.1461\n",
      "Epoch 183/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1455 - reg_loss: 0.0253 - clf_loss: 0.0902 - reg_mae: 0.1256 - val_loss: 0.4158 - val_reg_loss: 0.0616 - val_clf_loss: 0.3238 - val_reg_mae: 0.1700\n",
      "Epoch 184/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1424 - reg_loss: 0.0233 - clf_loss: 0.0886 - reg_mae: 0.1204 - val_loss: 0.3920 - val_reg_loss: 0.0525 - val_clf_loss: 0.3090 - val_reg_mae: 0.1470\n",
      "Epoch 185/3000\n",
      "38/38 [==============================] - 2s 52ms/step - loss: 0.1438 - reg_loss: 0.0245 - clf_loss: 0.0888 - reg_mae: 0.1234 - val_loss: 0.3975 - val_reg_loss: 0.0655 - val_clf_loss: 0.3018 - val_reg_mae: 0.1798\n",
      "Epoch 186/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1462 - reg_loss: 0.0247 - clf_loss: 0.0910 - reg_mae: 0.1224 - val_loss: 0.3933 - val_reg_loss: 0.0651 - val_clf_loss: 0.2972 - val_reg_mae: 0.1640\n",
      "Epoch 187/3000\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1510 - reg_loss: 0.0246 - clf_loss: 0.0951 - reg_mae: 0.1237 - val_loss: 0.3895 - val_reg_loss: 0.0565 - val_clf_loss: 0.3020 - val_reg_mae: 0.1530\n",
      "Epoch 188/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1629 - reg_loss: 0.0260 - clf_loss: 0.1040 - reg_mae: 0.1279 - val_loss: 0.4103 - val_reg_loss: 0.0604 - val_clf_loss: 0.3163 - val_reg_mae: 0.1550\n",
      "Epoch 189/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1526 - reg_loss: 0.0266 - clf_loss: 0.0926 - reg_mae: 0.1277 - val_loss: 0.4005 - val_reg_loss: 0.0594 - val_clf_loss: 0.3075 - val_reg_mae: 0.1732\n",
      "Epoch 190/3000\n",
      "38/38 [==============================] - 2s 54ms/step - loss: 0.1548 - reg_loss: 0.0254 - clf_loss: 0.0957 - reg_mae: 0.1271 - val_loss: 0.4045 - val_reg_loss: 0.0561 - val_clf_loss: 0.3144 - val_reg_mae: 0.1555\n",
      "Epoch 191/3000\n",
      "38/38 [==============================] - 2s 55ms/step - loss: 0.1728 - reg_loss: 0.0273 - clf_loss: 0.1084 - reg_mae: 0.1311 - val_loss: 0.4228 - val_reg_loss: 0.0563 - val_clf_loss: 0.3284 - val_reg_mae: 0.1605\n",
      "Epoch 192/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1509 - reg_loss: 0.0252 - clf_loss: 0.0890 - reg_mae: 0.1273 - val_loss: 0.4060 - val_reg_loss: 0.0548 - val_clf_loss: 0.3162 - val_reg_mae: 0.1567\n",
      "Epoch 193/3000\n",
      "38/38 [==============================] - 2s 50ms/step - loss: 0.1447 - reg_loss: 0.0244 - clf_loss: 0.0859 - reg_mae: 0.1242 - val_loss: 0.4089 - val_reg_loss: 0.0576 - val_clf_loss: 0.3174 - val_reg_mae: 0.1598\n",
      "Epoch 194/3000\n",
      "38/38 [==============================] - 2s 51ms/step - loss: 0.1318 - reg_loss: 0.0227 - clf_loss: 0.0763 - reg_mae: 0.1178 - val_loss: 0.3869 - val_reg_loss: 0.0549 - val_clf_loss: 0.3004 - val_reg_mae: 0.1661\n",
      "Epoch 195/3000\n",
      "38/38 [==============================] - 2s 53ms/step - loss: 0.1336 - reg_loss: 0.0232 - clf_loss: 0.0790 - reg_mae: 0.1213 - val_loss: 0.4184 - val_reg_loss: 0.0529 - val_clf_loss: 0.3345 - val_reg_mae: 0.1561\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "# tensor_board = TensorBoard(log_dir=f\"log/{model_comment}/{TIMESTAMP}/\", histogram_freq=1)\n",
    "# tensor_board = TensorBoard(log_dir=f\"log/{model_comment}/test_run_{TIMESTAMP}_lstm/\", histogram_freq=1)\n",
    "early_stopping = EarlyStopping(patience=30, monitor=\"val_reg_mae\", mode=\"min\", restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    **test_run_data,\n",
    "    # **fold_1,\n",
    "    epochs=3000,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stopping, tensor_board],\n",
    "    shuffle=False,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# model.save(f\"models/{model_comment}_{TIMESTAMP}.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "76\n",
      "121\n",
      "200\n",
      "279\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from keras.callbacks import EarlyStopping, TensorBoard\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%m%d_%H%M\")\n",
    "tensor_board = TensorBoard(log_dir=f\"log/{model_comment}_{TIMESTAMP}/\", histogram_freq=1)\n",
    "early_stopping = EarlyStopping(patience=30, monitor=\"val_reg_mae\", mode=\"min\", restore_best_weights=True)\n",
    "\n",
    "splits_ = SplitData(window)\n",
    "last_epoch = 0\n",
    "for i, fold in enumerate(splits_.train_valid_split()):\n",
    "    print(last_epoch)\n",
    "    history = model.fit(\n",
    "        **fold,\n",
    "        epochs=300,\n",
    "        batch_size=32,\n",
    "        initial_epoch=last_epoch,\n",
    "        callbacks=[early_stopping, tensor_board],\n",
    "        shuffle=False,\n",
    "        verbose=0,\n",
    "    )\n",
    "    last_epoch += len(history.epoch)\n",
    "\n",
    "    model.save(f\"models/{model_comment}_{TIMESTAMP}_{i}.keras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126/126 [==============================] - 2s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(fold_1[\"validation_data\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = load_y_pred(\"models/y_true.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: complete the function\n",
    "def save_model_pred(): pass\n",
    "y_pred[0] = splits.denormalize_fare(y_pred[0]).squeeze()\n",
    "y_pred[1] = y_pred[1].argmax(axis=-1)\n",
    "np.savez_compressed(\"simple_LSTM_1115_0002.npz\", fare=y_pred[0], seat=y_pred[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dict(zip([\"fare\", \"seat\"], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33.45066234794296, 0.44249303919468835)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: read pred result and draw picture\n",
    "_len = y_pred[\"fare\"].shape[0]\n",
    "mean_absolute_error(y_true[\"fare\"][:_len], y_pred[\"fare\"]), mean_absolute_error(y_true[\"seat\"][:_len], y_pred[\"seat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### keep training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_epoch += len(history.epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 339/1000\n",
      "38/38 [==============================] - 2s 35ms/step - loss: 0.4026 - reg_loss: 0.0473 - clf_loss: 0.2521 - reg_mae: 0.1654 - clf_accuracy: 0.9194 - val_loss: 0.6345 - val_reg_loss: 0.0835 - val_clf_loss: 0.4497 - val_reg_mae: 0.1824 - val_clf_accuracy: 0.8905\n",
      "Epoch 340/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4047 - reg_loss: 0.0494 - clf_loss: 0.2538 - reg_mae: 0.1686 - clf_accuracy: 0.9196 - val_loss: 0.6413 - val_reg_loss: 0.0861 - val_clf_loss: 0.4496 - val_reg_mae: 0.1897 - val_clf_accuracy: 0.8905\n",
      "Epoch 341/1000\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 0.4118 - reg_loss: 0.0478 - clf_loss: 0.2576 - reg_mae: 0.1666 - clf_accuracy: 0.9187 - val_loss: 0.6514 - val_reg_loss: 0.0828 - val_clf_loss: 0.4600 - val_reg_mae: 0.1885 - val_clf_accuracy: 0.8821\n",
      "Epoch 342/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4068 - reg_loss: 0.0472 - clf_loss: 0.2532 - reg_mae: 0.1676 - clf_accuracy: 0.9185 - val_loss: 0.6285 - val_reg_loss: 0.0709 - val_clf_loss: 0.4519 - val_reg_mae: 0.1723 - val_clf_accuracy: 0.8881\n",
      "Epoch 343/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4439 - reg_loss: 0.0499 - clf_loss: 0.2658 - reg_mae: 0.1705 - clf_accuracy: 0.9158 - val_loss: 0.6760 - val_reg_loss: 0.0779 - val_clf_loss: 0.4582 - val_reg_mae: 0.1819 - val_clf_accuracy: 0.8893\n",
      "Epoch 344/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4213 - reg_loss: 0.0498 - clf_loss: 0.2509 - reg_mae: 0.1702 - clf_accuracy: 0.9168 - val_loss: 0.6560 - val_reg_loss: 0.0951 - val_clf_loss: 0.4489 - val_reg_mae: 0.1900 - val_clf_accuracy: 0.8917\n",
      "Epoch 345/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4182 - reg_loss: 0.0575 - clf_loss: 0.2518 - reg_mae: 0.1816 - clf_accuracy: 0.9182 - val_loss: 0.6808 - val_reg_loss: 0.1064 - val_clf_loss: 0.4579 - val_reg_mae: 0.2190 - val_clf_accuracy: 0.8893\n",
      "Epoch 346/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4089 - reg_loss: 0.0494 - clf_loss: 0.2509 - reg_mae: 0.1722 - clf_accuracy: 0.9174 - val_loss: 0.6350 - val_reg_loss: 0.0796 - val_clf_loss: 0.4514 - val_reg_mae: 0.1773 - val_clf_accuracy: 0.8917\n",
      "Epoch 347/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4080 - reg_loss: 0.0499 - clf_loss: 0.2520 - reg_mae: 0.1717 - clf_accuracy: 0.9194 - val_loss: 0.6855 - val_reg_loss: 0.1043 - val_clf_loss: 0.4730 - val_reg_mae: 0.2044 - val_clf_accuracy: 0.8786\n",
      "Epoch 348/1000\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 0.4168 - reg_loss: 0.0486 - clf_loss: 0.2589 - reg_mae: 0.1680 - clf_accuracy: 0.9171 - val_loss: 0.6472 - val_reg_loss: 0.0796 - val_clf_loss: 0.4576 - val_reg_mae: 0.1885 - val_clf_accuracy: 0.8845\n",
      "Epoch 349/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4601 - reg_loss: 0.0587 - clf_loss: 0.2644 - reg_mae: 0.1855 - clf_accuracy: 0.9124 - val_loss: 0.7296 - val_reg_loss: 0.0953 - val_clf_loss: 0.4768 - val_reg_mae: 0.1952 - val_clf_accuracy: 0.8810\n",
      "Epoch 350/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4449 - reg_loss: 0.0507 - clf_loss: 0.2599 - reg_mae: 0.1735 - clf_accuracy: 0.9170 - val_loss: 0.6516 - val_reg_loss: 0.0827 - val_clf_loss: 0.4522 - val_reg_mae: 0.1745 - val_clf_accuracy: 0.8869\n",
      "Epoch 351/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.4046 - reg_loss: 0.0469 - clf_loss: 0.2479 - reg_mae: 0.1657 - clf_accuracy: 0.9189 - val_loss: 0.6403 - val_reg_loss: 0.0870 - val_clf_loss: 0.4449 - val_reg_mae: 0.1776 - val_clf_accuracy: 0.8905\n",
      "Epoch 352/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3985 - reg_loss: 0.0449 - clf_loss: 0.2484 - reg_mae: 0.1609 - clf_accuracy: 0.9201 - val_loss: 0.6385 - val_reg_loss: 0.0784 - val_clf_loss: 0.4551 - val_reg_mae: 0.1793 - val_clf_accuracy: 0.8905\n",
      "Epoch 353/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4096 - reg_loss: 0.0492 - clf_loss: 0.2535 - reg_mae: 0.1670 - clf_accuracy: 0.9181 - val_loss: 0.6581 - val_reg_loss: 0.0827 - val_clf_loss: 0.4618 - val_reg_mae: 0.1782 - val_clf_accuracy: 0.8810\n",
      "Epoch 354/1000\n",
      "38/38 [==============================] - 1s 34ms/step - loss: 0.4055 - reg_loss: 0.0501 - clf_loss: 0.2481 - reg_mae: 0.1695 - clf_accuracy: 0.9192 - val_loss: 0.6693 - val_reg_loss: 0.0943 - val_clf_loss: 0.4619 - val_reg_mae: 0.1896 - val_clf_accuracy: 0.8881\n",
      "Epoch 355/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4078 - reg_loss: 0.0472 - clf_loss: 0.2529 - reg_mae: 0.1640 - clf_accuracy: 0.9180 - val_loss: 0.6430 - val_reg_loss: 0.0726 - val_clf_loss: 0.4566 - val_reg_mae: 0.1725 - val_clf_accuracy: 0.8857\n",
      "Epoch 356/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4130 - reg_loss: 0.0503 - clf_loss: 0.2514 - reg_mae: 0.1689 - clf_accuracy: 0.9179 - val_loss: 0.6649 - val_reg_loss: 0.0963 - val_clf_loss: 0.4571 - val_reg_mae: 0.1999 - val_clf_accuracy: 0.8893\n",
      "Epoch 357/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4996 - reg_loss: 0.0758 - clf_loss: 0.2873 - reg_mae: 0.2046 - clf_accuracy: 0.9105 - val_loss: 0.7344 - val_reg_loss: 0.1148 - val_clf_loss: 0.4775 - val_reg_mae: 0.2327 - val_clf_accuracy: 0.8810\n",
      "Epoch 358/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4366 - reg_loss: 0.0539 - clf_loss: 0.2603 - reg_mae: 0.1812 - clf_accuracy: 0.9179 - val_loss: 0.6604 - val_reg_loss: 0.0814 - val_clf_loss: 0.4626 - val_reg_mae: 0.1804 - val_clf_accuracy: 0.8893\n",
      "Epoch 359/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4070 - reg_loss: 0.0500 - clf_loss: 0.2479 - reg_mae: 0.1674 - clf_accuracy: 0.9187 - val_loss: 0.6220 - val_reg_loss: 0.0710 - val_clf_loss: 0.4476 - val_reg_mae: 0.1704 - val_clf_accuracy: 0.8893\n",
      "Epoch 360/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3912 - reg_loss: 0.0465 - clf_loss: 0.2434 - reg_mae: 0.1622 - clf_accuracy: 0.9199 - val_loss: 0.6318 - val_reg_loss: 0.0775 - val_clf_loss: 0.4507 - val_reg_mae: 0.1780 - val_clf_accuracy: 0.8881\n",
      "Epoch 361/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3981 - reg_loss: 0.0476 - clf_loss: 0.2467 - reg_mae: 0.1651 - clf_accuracy: 0.9205 - val_loss: 0.6418 - val_reg_loss: 0.0808 - val_clf_loss: 0.4543 - val_reg_mae: 0.1841 - val_clf_accuracy: 0.8893\n",
      "Epoch 362/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3934 - reg_loss: 0.0471 - clf_loss: 0.2432 - reg_mae: 0.1643 - clf_accuracy: 0.9196 - val_loss: 0.6463 - val_reg_loss: 0.0877 - val_clf_loss: 0.4515 - val_reg_mae: 0.1911 - val_clf_accuracy: 0.8905\n",
      "Epoch 363/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3914 - reg_loss: 0.0476 - clf_loss: 0.2411 - reg_mae: 0.1655 - clf_accuracy: 0.9210 - val_loss: 0.6284 - val_reg_loss: 0.0769 - val_clf_loss: 0.4484 - val_reg_mae: 0.1773 - val_clf_accuracy: 0.8881\n",
      "Epoch 364/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4012 - reg_loss: 0.0506 - clf_loss: 0.2465 - reg_mae: 0.1696 - clf_accuracy: 0.9193 - val_loss: 0.6377 - val_reg_loss: 0.0816 - val_clf_loss: 0.4496 - val_reg_mae: 0.1917 - val_clf_accuracy: 0.8869\n",
      "Epoch 365/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4016 - reg_loss: 0.0497 - clf_loss: 0.2453 - reg_mae: 0.1701 - clf_accuracy: 0.9188 - val_loss: 0.6438 - val_reg_loss: 0.0738 - val_clf_loss: 0.4591 - val_reg_mae: 0.1769 - val_clf_accuracy: 0.8893\n",
      "Epoch 366/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3927 - reg_loss: 0.0451 - clf_loss: 0.2439 - reg_mae: 0.1603 - clf_accuracy: 0.9195 - val_loss: 0.6491 - val_reg_loss: 0.0912 - val_clf_loss: 0.4569 - val_reg_mae: 0.1898 - val_clf_accuracy: 0.8893\n",
      "Epoch 367/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.3844 - reg_loss: 0.0467 - clf_loss: 0.2377 - reg_mae: 0.1643 - clf_accuracy: 0.9210 - val_loss: 0.6401 - val_reg_loss: 0.0846 - val_clf_loss: 0.4535 - val_reg_mae: 0.1814 - val_clf_accuracy: 0.8881\n",
      "Epoch 368/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4111 - reg_loss: 0.0510 - clf_loss: 0.2483 - reg_mae: 0.1680 - clf_accuracy: 0.9177 - val_loss: 0.6723 - val_reg_loss: 0.0883 - val_clf_loss: 0.4621 - val_reg_mae: 0.2031 - val_clf_accuracy: 0.8845\n",
      "Epoch 369/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4128 - reg_loss: 0.0470 - clf_loss: 0.2503 - reg_mae: 0.1661 - clf_accuracy: 0.9182 - val_loss: 0.6903 - val_reg_loss: 0.1036 - val_clf_loss: 0.4724 - val_reg_mae: 0.2048 - val_clf_accuracy: 0.8786\n",
      "Epoch 370/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4038 - reg_loss: 0.0466 - clf_loss: 0.2467 - reg_mae: 0.1631 - clf_accuracy: 0.9182 - val_loss: 0.6628 - val_reg_loss: 0.0977 - val_clf_loss: 0.4570 - val_reg_mae: 0.1939 - val_clf_accuracy: 0.8893\n",
      "Epoch 371/1000\n",
      "38/38 [==============================] - 1s 33ms/step - loss: 0.3904 - reg_loss: 0.0466 - clf_loss: 0.2396 - reg_mae: 0.1646 - clf_accuracy: 0.9199 - val_loss: 0.6345 - val_reg_loss: 0.0836 - val_clf_loss: 0.4474 - val_reg_mae: 0.1747 - val_clf_accuracy: 0.8881\n",
      "Epoch 372/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.3927 - reg_loss: 0.0475 - clf_loss: 0.2413 - reg_mae: 0.1635 - clf_accuracy: 0.9204 - val_loss: 0.6467 - val_reg_loss: 0.0756 - val_clf_loss: 0.4649 - val_reg_mae: 0.1717 - val_clf_accuracy: 0.8857\n",
      "Epoch 373/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4104 - reg_loss: 0.0518 - clf_loss: 0.2471 - reg_mae: 0.1724 - clf_accuracy: 0.9202 - val_loss: 0.6589 - val_reg_loss: 0.0795 - val_clf_loss: 0.4673 - val_reg_mae: 0.1830 - val_clf_accuracy: 0.8833\n",
      "Epoch 374/1000\n",
      "38/38 [==============================] - 1s 32ms/step - loss: 0.4015 - reg_loss: 0.0486 - clf_loss: 0.2431 - reg_mae: 0.1645 - clf_accuracy: 0.9194 - val_loss: 0.6219 - val_reg_loss: 0.0700 - val_clf_loss: 0.4446 - val_reg_mae: 0.1786 - val_clf_accuracy: 0.8905\n",
      "Epoch 375/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3895 - reg_loss: 0.0453 - clf_loss: 0.2411 - reg_mae: 0.1620 - clf_accuracy: 0.9189 - val_loss: 0.6342 - val_reg_loss: 0.0802 - val_clf_loss: 0.4533 - val_reg_mae: 0.1785 - val_clf_accuracy: 0.8893\n",
      "Epoch 376/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3897 - reg_loss: 0.0483 - clf_loss: 0.2398 - reg_mae: 0.1644 - clf_accuracy: 0.9200 - val_loss: 0.6328 - val_reg_loss: 0.0760 - val_clf_loss: 0.4499 - val_reg_mae: 0.1730 - val_clf_accuracy: 0.8893\n",
      "Epoch 377/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4004 - reg_loss: 0.0524 - clf_loss: 0.2441 - reg_mae: 0.1730 - clf_accuracy: 0.9206 - val_loss: 0.6307 - val_reg_loss: 0.0653 - val_clf_loss: 0.4536 - val_reg_mae: 0.1648 - val_clf_accuracy: 0.8917\n",
      "Epoch 378/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4071 - reg_loss: 0.0501 - clf_loss: 0.2458 - reg_mae: 0.1681 - clf_accuracy: 0.9187 - val_loss: 0.6587 - val_reg_loss: 0.0834 - val_clf_loss: 0.4590 - val_reg_mae: 0.1888 - val_clf_accuracy: 0.8821\n",
      "Epoch 379/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4282 - reg_loss: 0.0511 - clf_loss: 0.2536 - reg_mae: 0.1718 - clf_accuracy: 0.9179 - val_loss: 0.6498 - val_reg_loss: 0.0800 - val_clf_loss: 0.4486 - val_reg_mae: 0.1840 - val_clf_accuracy: 0.8881\n",
      "Epoch 380/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4070 - reg_loss: 0.0487 - clf_loss: 0.2448 - reg_mae: 0.1680 - clf_accuracy: 0.9196 - val_loss: 0.6307 - val_reg_loss: 0.0755 - val_clf_loss: 0.4472 - val_reg_mae: 0.1820 - val_clf_accuracy: 0.8905\n",
      "Epoch 381/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3927 - reg_loss: 0.0480 - clf_loss: 0.2402 - reg_mae: 0.1642 - clf_accuracy: 0.9196 - val_loss: 0.6324 - val_reg_loss: 0.0801 - val_clf_loss: 0.4487 - val_reg_mae: 0.1891 - val_clf_accuracy: 0.8917\n",
      "Epoch 382/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.3784 - reg_loss: 0.0463 - clf_loss: 0.2331 - reg_mae: 0.1644 - clf_accuracy: 0.9204 - val_loss: 0.6515 - val_reg_loss: 0.0765 - val_clf_loss: 0.4717 - val_reg_mae: 0.1719 - val_clf_accuracy: 0.8833\n",
      "Epoch 383/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3927 - reg_loss: 0.0446 - clf_loss: 0.2383 - reg_mae: 0.1589 - clf_accuracy: 0.9192 - val_loss: 0.6949 - val_reg_loss: 0.1225 - val_clf_loss: 0.4643 - val_reg_mae: 0.2157 - val_clf_accuracy: 0.8869\n",
      "Epoch 384/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4262 - reg_loss: 0.0513 - clf_loss: 0.2502 - reg_mae: 0.1732 - clf_accuracy: 0.9160 - val_loss: 0.7195 - val_reg_loss: 0.0920 - val_clf_loss: 0.4701 - val_reg_mae: 0.1844 - val_clf_accuracy: 0.8881\n",
      "Epoch 385/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4280 - reg_loss: 0.0482 - clf_loss: 0.2494 - reg_mae: 0.1656 - clf_accuracy: 0.9163 - val_loss: 0.6741 - val_reg_loss: 0.0898 - val_clf_loss: 0.4643 - val_reg_mae: 0.1973 - val_clf_accuracy: 0.8893\n",
      "Epoch 386/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4070 - reg_loss: 0.0522 - clf_loss: 0.2391 - reg_mae: 0.1716 - clf_accuracy: 0.9199 - val_loss: 0.6673 - val_reg_loss: 0.0952 - val_clf_loss: 0.4569 - val_reg_mae: 0.1959 - val_clf_accuracy: 0.8881\n",
      "Epoch 387/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3985 - reg_loss: 0.0491 - clf_loss: 0.2394 - reg_mae: 0.1673 - clf_accuracy: 0.9194 - val_loss: 0.6473 - val_reg_loss: 0.0787 - val_clf_loss: 0.4590 - val_reg_mae: 0.1822 - val_clf_accuracy: 0.8905\n",
      "Epoch 388/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.3974 - reg_loss: 0.0524 - clf_loss: 0.2381 - reg_mae: 0.1710 - clf_accuracy: 0.9201 - val_loss: 0.6410 - val_reg_loss: 0.0738 - val_clf_loss: 0.4559 - val_reg_mae: 0.1839 - val_clf_accuracy: 0.8845\n",
      "Epoch 389/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.3880 - reg_loss: 0.0473 - clf_loss: 0.2350 - reg_mae: 0.1653 - clf_accuracy: 0.9200 - val_loss: 0.6541 - val_reg_loss: 0.0965 - val_clf_loss: 0.4563 - val_reg_mae: 0.1931 - val_clf_accuracy: 0.8833\n",
      "Epoch 390/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3819 - reg_loss: 0.0475 - clf_loss: 0.2338 - reg_mae: 0.1648 - clf_accuracy: 0.9206 - val_loss: 0.6661 - val_reg_loss: 0.0940 - val_clf_loss: 0.4660 - val_reg_mae: 0.1896 - val_clf_accuracy: 0.8881\n",
      "Epoch 391/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3849 - reg_loss: 0.0471 - clf_loss: 0.2335 - reg_mae: 0.1644 - clf_accuracy: 0.9211 - val_loss: 0.6598 - val_reg_loss: 0.0949 - val_clf_loss: 0.4603 - val_reg_mae: 0.1927 - val_clf_accuracy: 0.8869\n",
      "Epoch 392/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.3865 - reg_loss: 0.0442 - clf_loss: 0.2360 - reg_mae: 0.1611 - clf_accuracy: 0.9183 - val_loss: 0.6563 - val_reg_loss: 0.0918 - val_clf_loss: 0.4601 - val_reg_mae: 0.1834 - val_clf_accuracy: 0.8905\n",
      "Epoch 393/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.3923 - reg_loss: 0.0485 - clf_loss: 0.2362 - reg_mae: 0.1634 - clf_accuracy: 0.9202 - val_loss: 0.7384 - val_reg_loss: 0.1742 - val_clf_loss: 0.4484 - val_reg_mae: 0.2400 - val_clf_accuracy: 0.8869\n",
      "Epoch 394/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3953 - reg_loss: 0.0526 - clf_loss: 0.2339 - reg_mae: 0.1719 - clf_accuracy: 0.9196 - val_loss: 0.6426 - val_reg_loss: 0.0708 - val_clf_loss: 0.4640 - val_reg_mae: 0.1696 - val_clf_accuracy: 0.8821\n",
      "Epoch 395/1000\n",
      "38/38 [==============================] - 1s 30ms/step - loss: 0.4005 - reg_loss: 0.0498 - clf_loss: 0.2389 - reg_mae: 0.1655 - clf_accuracy: 0.9204 - val_loss: 0.6261 - val_reg_loss: 0.0635 - val_clf_loss: 0.4526 - val_reg_mae: 0.1737 - val_clf_accuracy: 0.8905\n",
      "Epoch 396/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3736 - reg_loss: 0.0433 - clf_loss: 0.2274 - reg_mae: 0.1592 - clf_accuracy: 0.9207 - val_loss: 0.6534 - val_reg_loss: 0.0846 - val_clf_loss: 0.4673 - val_reg_mae: 0.1815 - val_clf_accuracy: 0.8821\n",
      "Epoch 397/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4525 - reg_loss: 0.0494 - clf_loss: 0.2702 - reg_mae: 0.1706 - clf_accuracy: 0.9117 - val_loss: 0.7236 - val_reg_loss: 0.0885 - val_clf_loss: 0.4830 - val_reg_mae: 0.1878 - val_clf_accuracy: 0.8810\n",
      "Epoch 398/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4435 - reg_loss: 0.0539 - clf_loss: 0.2503 - reg_mae: 0.1779 - clf_accuracy: 0.9177 - val_loss: 0.6664 - val_reg_loss: 0.0778 - val_clf_loss: 0.4625 - val_reg_mae: 0.1839 - val_clf_accuracy: 0.8905\n",
      "Epoch 399/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4085 - reg_loss: 0.0490 - clf_loss: 0.2427 - reg_mae: 0.1654 - clf_accuracy: 0.9177 - val_loss: 0.6689 - val_reg_loss: 0.0848 - val_clf_loss: 0.4618 - val_reg_mae: 0.1794 - val_clf_accuracy: 0.8881\n",
      "Epoch 400/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3940 - reg_loss: 0.0461 - clf_loss: 0.2339 - reg_mae: 0.1594 - clf_accuracy: 0.9211 - val_loss: 0.6434 - val_reg_loss: 0.0825 - val_clf_loss: 0.4531 - val_reg_mae: 0.1824 - val_clf_accuracy: 0.8929\n",
      "Epoch 401/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3788 - reg_loss: 0.0460 - clf_loss: 0.2286 - reg_mae: 0.1610 - clf_accuracy: 0.9206 - val_loss: 0.6476 - val_reg_loss: 0.0860 - val_clf_loss: 0.4594 - val_reg_mae: 0.1858 - val_clf_accuracy: 0.8917\n",
      "Epoch 402/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3823 - reg_loss: 0.0470 - clf_loss: 0.2325 - reg_mae: 0.1623 - clf_accuracy: 0.9211 - val_loss: 0.6627 - val_reg_loss: 0.0912 - val_clf_loss: 0.4603 - val_reg_mae: 0.1914 - val_clf_accuracy: 0.8845\n",
      "Epoch 403/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3844 - reg_loss: 0.0487 - clf_loss: 0.2288 - reg_mae: 0.1647 - clf_accuracy: 0.9208 - val_loss: 0.6650 - val_reg_loss: 0.0982 - val_clf_loss: 0.4601 - val_reg_mae: 0.1929 - val_clf_accuracy: 0.8881\n",
      "Epoch 404/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3774 - reg_loss: 0.0507 - clf_loss: 0.2239 - reg_mae: 0.1697 - clf_accuracy: 0.9213 - val_loss: 0.6443 - val_reg_loss: 0.0794 - val_clf_loss: 0.4607 - val_reg_mae: 0.1804 - val_clf_accuracy: 0.8845\n",
      "Epoch 405/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.3951 - reg_loss: 0.0494 - clf_loss: 0.2359 - reg_mae: 0.1664 - clf_accuracy: 0.9196 - val_loss: 0.6980 - val_reg_loss: 0.1090 - val_clf_loss: 0.4764 - val_reg_mae: 0.2177 - val_clf_accuracy: 0.8833\n",
      "Epoch 406/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.5083 - reg_loss: 0.0644 - clf_loss: 0.2724 - reg_mae: 0.1886 - clf_accuracy: 0.9138 - val_loss: 0.7376 - val_reg_loss: 0.0807 - val_clf_loss: 0.4733 - val_reg_mae: 0.1888 - val_clf_accuracy: 0.8929\n",
      "Epoch 407/1000\n",
      "38/38 [==============================] - 1s 31ms/step - loss: 0.4411 - reg_loss: 0.0511 - clf_loss: 0.2416 - reg_mae: 0.1724 - clf_accuracy: 0.9180 - val_loss: 0.6901 - val_reg_loss: 0.0889 - val_clf_loss: 0.4734 - val_reg_mae: 0.1898 - val_clf_accuracy: 0.8869\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    **test_run_data,\n",
    "    epochs=1000,\n",
    "    batch_size=32,\n",
    "    initial_epoch=last_epoch,\n",
    "    callbacks=[early_stopping, tensor_board],\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show allocated memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392.50 MB\n"
     ]
    }
   ],
   "source": [
    "from psutil import Process\n",
    "\n",
    "print(f\"{Process().memory_info().rss / 2 ** 20:.2f} MB\")\n",
    "# Process().memory_percent()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
